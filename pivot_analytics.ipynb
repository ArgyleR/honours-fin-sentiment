{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x285bf507210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"./output_temp.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to flatten nested dictionary (dataset_params and model_params)\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process the JSON data into a DataFrame\n",
    "def process_data(data):\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        # Flatten model_params and dataset_params and combine them\n",
    "        flattened_params = {**flatten_dict(entry[\"model_params\"]), **flatten_dict(entry[\"dataset_params\"])}\n",
    "        \n",
    "        # Add train and validation metrics\n",
    "        train_metrics = entry[\"train_metrics\"]\n",
    "        val_metrics = entry[\"val_metrics\"]\n",
    "\n",
    "        for epoch in range(entry[\"epochs\"]):\n",
    "            row = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_metrics['loss'][epoch],\n",
    "                'train_accuracy': train_metrics['accuracy'][epoch],\n",
    "                'train_f1': train_metrics['f1'][epoch],\n",
    "                'val_loss': val_metrics['loss'][epoch],\n",
    "                'val_accuracy': val_metrics['accuracy'][epoch],\n",
    "                'val_f1': val_metrics['f1'][epoch],\n",
    "            }\n",
    "            row.update(flattened_params)  # Include all flattened parameters in the row\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "df = process_data(data)\n",
    "\n",
    "# Get all unique parameter names (excluding metric columns)\n",
    "pivot_options = [col for col in df.columns if col not in ['epoch', 'train_loss', 'train_accuracy', 'train_f1', 'val_loss', 'val_accuracy', 'val_f1']]\n",
    "\n",
    "# Create a Dash app for interactive plotting\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Model Training Metrics\"),\n",
    "    \n",
    "    # Dropdown for selecting the metric to display\n",
    "    html.Label(\"Choose a Metric:\"),\n",
    "    dcc.Dropdown(\n",
    "        id=\"metric\",\n",
    "        options=[\n",
    "            {'label': 'Loss', 'value': 'loss'},\n",
    "            {'label': 'Accuracy', 'value': 'accuracy'},\n",
    "            {'label': 'F1 Score', 'value': 'f1'},\n",
    "        ],\n",
    "        value='loss'\n",
    "    ),\n",
    "    \n",
    "    # Dropdown for selecting the hyperparameter to pivot over (dynamic based on dataset)\n",
    "    html.Label(\"Choose a Hyperparameter to Pivot Over:\"),\n",
    "    dcc.Dropdown(\n",
    "        id=\"pivot_by\",\n",
    "        options=[{'label': col.replace('_', ' ').capitalize(), 'value': col} for col in pivot_options],\n",
    "        value='learning_rate'\n",
    "    ),\n",
    "\n",
    "    # Toggle to switch between individual performance and summary view\n",
    "    dcc.RadioItems(\n",
    "        id='view_mode',\n",
    "        options=[\n",
    "            {'label': 'Individual', 'value': 'individual'},\n",
    "            {'label': 'Summary', 'value': 'summary'}\n",
    "        ],\n",
    "        value='individual',\n",
    "        labelStyle={'display': 'inline-block'}\n",
    "    ),\n",
    "    \n",
    "    # Plot for training metrics\n",
    "    dcc.Graph(id='train_metric_graph'),\n",
    "    \n",
    "    # Plot for validation metrics\n",
    "    dcc.Graph(id='val_metric_graph')\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('train_metric_graph', 'figure'),\n",
    "     Output('val_metric_graph', 'figure')],\n",
    "    [Input('metric', 'value'),\n",
    "     Input('pivot_by', 'value'),\n",
    "     Input('view_mode', 'value')]\n",
    ")\n",
    "def update_graph(selected_metric, pivot_by, view_mode):\n",
    "    # Columns for train and validation metrics\n",
    "    train_metric_col = f\"train_{selected_metric}\"\n",
    "    val_metric_col = f\"val_{selected_metric}\"\n",
    "\n",
    "    if view_mode == 'summary':\n",
    "        # Calculate summary for each epoch (min, max, avg) for each value of pivot_by\n",
    "        summary_df = df.groupby(['epoch', pivot_by]).agg(\n",
    "            avg_train_metric=(train_metric_col, 'mean'),\n",
    "            min_train_metric=(train_metric_col, 'min'),\n",
    "            max_train_metric=(train_metric_col, 'max'),\n",
    "            avg_val_metric=(val_metric_col, 'mean'),\n",
    "            min_val_metric=(val_metric_col, 'min'),\n",
    "            max_val_metric=(val_metric_col, 'max')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Find min/max models for each epoch\n",
    "        min_train_df = df.loc[df.groupby(['epoch'])[train_metric_col].idxmin()]\n",
    "        max_train_df = df.loc[df.groupby(['epoch'])[train_metric_col].idxmax()]\n",
    "        \n",
    "        min_val_df = df.loc[df.groupby(['epoch'])[val_metric_col].idxmin()]\n",
    "        max_val_df = df.loc[df.groupby(['epoch'])[val_metric_col].idxmax()]\n",
    "\n",
    "        # Plot average, min, and max for training metrics\n",
    "        train_fig = px.line(summary_df, x='epoch', y='avg_train_metric', color=pivot_by,\n",
    "                            title=f\"Train {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "                            labels={'epoch': 'Epoch', 'avg_train_metric': f\"Average Train {selected_metric.capitalize()}\"})\n",
    "        \n",
    "        # Add min and max as dashed lines (using actual models with min/max)\n",
    "        train_fig.add_scatter(x=min_train_df['epoch'], y=min_train_df[train_metric_col], mode='lines', name='Min Train', line=dict(dash='dash'))\n",
    "        train_fig.add_scatter(x=max_train_df['epoch'], y=max_train_df[train_metric_col], mode='lines', name='Max Train', line=dict(dash='dash'))\n",
    "\n",
    "        # Plot average, min, and max for validation metrics\n",
    "        val_fig = px.line(summary_df, x='epoch', y='avg_val_metric', color=pivot_by,\n",
    "                          title=f\"Validation {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "                          labels={'epoch': 'Epoch', 'avg_val_metric': f\"Average Validation {selected_metric.capitalize()}\"})\n",
    "\n",
    "        # Add min and max as dashed lines (using actual models with min/max)\n",
    "        val_fig.add_scatter(x=min_val_df['epoch'], y=min_val_df[val_metric_col], mode='lines', name='Min Val', line=dict(dash='dash'))\n",
    "        val_fig.add_scatter(x=max_val_df['epoch'], y=max_val_df[val_metric_col], mode='lines', name='Max Val', line=dict(dash='dash'))\n",
    "\n",
    "    else:  # Individual mode\n",
    "        # Plot for training metrics (individual performance)\n",
    "        train_fig = px.line(df, x=\"epoch\", y=train_metric_col, color=pivot_by, markers=True,\n",
    "                            labels={\n",
    "                                \"epoch\": \"Epoch\",\n",
    "                                train_metric_col: f\"Train {selected_metric.capitalize()}\",\n",
    "                                pivot_by: pivot_by.replace(\"_\", \" \").capitalize()\n",
    "                            },\n",
    "                            title=f\"Train {selected_metric.capitalize()} Over Epochs\")\n",
    "        \n",
    "        # Plot for validation metrics (individual performance)\n",
    "        val_fig = px.line(df, x=\"epoch\", y=val_metric_col, color=pivot_by, markers=True,\n",
    "                          labels={\n",
    "                              \"epoch\": \"Epoch\",\n",
    "                              val_metric_col: f\"Validation {selected_metric.capitalize()}\",\n",
    "                              pivot_by: pivot_by.replace(\"_\", \" \").capitalize()\n",
    "                          },\n",
    "                          title=f\"Validation {selected_metric.capitalize()} Over Epochs\")\n",
    "\n",
    "    return train_fig, val_fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f18247c790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[1], line 205, in update_graph(\n",
      "    selected_metric='loss',\n",
      "    pivot_by='text_selection_method',\n",
      "    view_mode='individual'\n",
      ")\n",
      "    197     val_fig.update_layout(\n",
      "    198         title=f\"Validation {selected_metric.capitalize()} Summary Over Epochs\",\n",
      "    199         xaxis_title=\"Epoch\",\n",
      "    200         yaxis_title=f\"Validation {selected_metric.capitalize()}\"\n",
      "    201     )\n",
      "    203 else:  # Individual mode\n",
      "    204     # Plot for training metrics (individual performance)\n",
      "--> 205     train_fig = px.line(df, x=\"epoch\", y=train_metric_col, color=pivot_by, markers=True,\n",
      "        train_metric_col = 'train_loss'\n",
      "        df =       epoch  train_loss  train_accuracy  train_f1  val_loss  val_accuracy  \\\n",
      "0         1    0.515992        0.507607  0.377019  0.082439      0.477778   \n",
      "1         2    0.501661        0.510373  0.344923  0.085130      0.477778   \n",
      "2         3    0.502206        0.510373  0.344923  0.086253      0.477778   \n",
      "3         4    0.496719        0.510373  0.344923  0.086030      0.477778   \n",
      "4         5    0.496294        0.510373  0.344923  0.086719      0.477778   \n",
      "...     ...         ...             ...       ...       ...           ...   \n",
      "2020      1    0.500901        0.511757  0.387931  0.080100      0.577778   \n",
      "2021      2    0.507910        0.497925  0.331031  0.094164      0.577778   \n",
      "2022      3    0.505884        0.497925  0.331031  0.093335      0.577778   \n",
      "2023      4    0.504886        0.493776  0.343227  0.093252      0.577778   \n",
      "2024      5    0.502507        0.497925  0.331031  0.092409      0.577778   \n",
      "\n",
      "        val_f1             ts_encoder_name  ts_encoder_ts_window  \\\n",
      "0     0.308939  TimeSeriesTransformerModel                    10   \n",
      "1     0.308939  TimeSeriesTransformerModel                    10   \n",
      "2     0.308939  TimeSeriesTransformerModel                    10   \n",
      "3     0.308939  TimeSeriesTransformerModel                    10   \n",
      "4     0.308939  TimeSeriesTransformerModel                    10   \n",
      "...        ...                         ...                   ...   \n",
      "2020  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2021  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2022  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2023  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2024  0.423161  TimeSeriesTransformerModel                    10   \n",
      "\n",
      "      ts_encoder_context_length  ...  text_window text_selection_method  \\\n",
      "0                             1  ...            1            [TFIDF, 5]   \n",
      "1                             1  ...            1            [TFIDF, 5]   \n",
      "2                             1  ...            1            [TFIDF, 5]   \n",
      "3                             1  ...            1            [TFIDF, 5]   \n",
      "4                             1  ...            1            [TFIDF, 5]   \n",
      "...                         ...  ...          ...                   ...   \n",
      "2020                          1  ...            1            [TFIDF, 5]   \n",
      "2021                          1  ...            1            [TFIDF, 5]   \n",
      "2022                          1  ...            1            [TFIDF, 5]   \n",
      "2023                          1  ...            1            [TFIDF, 5]   \n",
      "2024                          1  ...            1            [TFIDF, 5]   \n",
      "\n",
      "      data_source_name                              data_source_text_path  \\\n",
      "0        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "1        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "3        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "4        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "...                ...                                                ...   \n",
      "2020     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2021     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2022     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2023     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2024     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "\n",
      "               data_source_ts_path data_source_ts_date_col  \\\n",
      "0     ./data/stock_emotions/price/                    Date   \n",
      "1     ./data/stock_emotions/price/                    Date   \n",
      "2     ./data/stock_emotions/price/                    Date   \n",
      "3     ./data/stock_emotions/price/                    Date   \n",
      "4     ./data/stock_emotions/price/                    Date   \n",
      "...                            ...                     ...   \n",
      "2020  ./data/stock_emotions/price/                    Date   \n",
      "2021  ./data/stock_emotions/price/                    Date   \n",
      "2022  ./data/stock_emotions/price/                    Date   \n",
      "2023  ./data/stock_emotions/price/                    Date   \n",
      "2024  ./data/stock_emotions/price/                    Date   \n",
      "\n",
      "      data_source_text_date_col  data_source_text_col negatives_creation  \\\n",
      "0                          date                  text        [naive, 60]   \n",
      "1                          date                  text        [naive, 60]   \n",
      "2                          date                  text        [naive, 60]   \n",
      "3                          date                  text        [naive, 60]   \n",
      "4                          date                  text        [naive, 60]   \n",
      "...                         ...                   ...                ...   \n",
      "2020                       date                  text        [naive, 60]   \n",
      "2021                       date                  text        [naive, 60]   \n",
      "2022                       date                  text        [naive, 60]   \n",
      "2023                       date                  text        [naive, 60]   \n",
      "2024                       date                  text        [naive, 60]   \n",
      "\n",
      "     random_state  \n",
      "0              42  \n",
      "1              42  \n",
      "2              42  \n",
      "3              42  \n",
      "4              42  \n",
      "...           ...  \n",
      "2020           43  \n",
      "2021           43  \n",
      "2022           43  \n",
      "2023           43  \n",
      "2024           43  \n",
      "\n",
      "[2025 rows x 35 columns]\n",
      "        px = <module 'plotly.express' from 'c:\\\\Users\\\\eoinp\\\\anaconda3\\\\envs\\\\deepl\\\\Lib\\\\site-packages\\\\plotly\\\\express\\\\__init__.py'>\n",
      "        pivot_by = 'text_selection_method'\n",
      "        selected_metric = 'loss'\n",
      "    206                         labels={\n",
      "    207                             \"epoch\": \"Epoch\",\n",
      "    208                             train_metric_col: f\"Train {selected_metric.capitalize()}\",\n",
      "    209                             pivot_by: pivot_by.replace(\"_\", \" \").capitalize()\n",
      "    210                         },\n",
      "    211                         title=f\"Train {selected_metric.capitalize()} Over Epochs\")\n",
      "    213     # Plot for validation metrics (individual performance)\n",
      "    214     val_fig = px.line(df, x=\"epoch\", y=val_metric_col, color=pivot_by, markers=True,\n",
      "    215                       labels={\n",
      "    216                           \"epoch\": \"Epoch\",\n",
      "   (...)\n",
      "    219                       },\n",
      "    220                       title=f\"Validation {selected_metric.capitalize()} Over Epochs\")\n",
      "\n",
      "File c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\plotly\\express\\_chart_types.py:264, in line(\n",
      "    data_frame=      epoch  train_loss  train_accuracy  train_f...3  \n",
      "2024           43  \n",
      "\n",
      "[2025 rows x 35 columns],\n",
      "    x='epoch',\n",
      "    y='train_loss',\n",
      "    line_group=None,\n",
      "    color='text_selection_method',\n",
      "    line_dash=None,\n",
      "    symbol=None,\n",
      "    hover_name=None,\n",
      "    hover_data=None,\n",
      "    custom_data=None,\n",
      "    text=None,\n",
      "    facet_row=None,\n",
      "    facet_col=None,\n",
      "    facet_col_wrap=0,\n",
      "    facet_row_spacing=None,\n",
      "    facet_col_spacing=None,\n",
      "    error_x=None,\n",
      "    error_x_minus=None,\n",
      "    error_y=None,\n",
      "    error_y_minus=None,\n",
      "    animation_frame=None,\n",
      "    animation_group=None,\n",
      "    category_orders=None,\n",
      "    labels={'epoch': 'Epoch', 'text_selection_method': 'Text selection method', 'train_loss': 'Train Loss'},\n",
      "    orientation=None,\n",
      "    color_discrete_sequence=None,\n",
      "    color_discrete_map=None,\n",
      "    line_dash_sequence=None,\n",
      "    line_dash_map=None,\n",
      "    symbol_sequence=None,\n",
      "    symbol_map=None,\n",
      "    markers=True,\n",
      "    log_x=False,\n",
      "    log_y=False,\n",
      "    range_x=None,\n",
      "    range_y=None,\n",
      "    line_shape=None,\n",
      "    render_mode='auto',\n",
      "    title='Train Loss Over Epochs',\n",
      "    template=None,\n",
      "    width=None,\n",
      "    height=None\n",
      ")\n",
      "    216 def line(\n",
      "    217     data_frame=None,\n",
      "    218     x=None,\n",
      "   (...)\n",
      "    258     height=None,\n",
      "    259 ) -> go.Figure:\n",
      "    260     \"\"\"\n",
      "    261     In a 2D line plot, each row of `data_frame` is represented as vertex of\n",
      "    262     a polyline mark in 2D space.\n",
      "    263     \"\"\"\n",
      "--> 264     return make_figure(args=locals(), constructor=go.Scatter)\n",
      "        go = <module 'plotly.graph_objs' from 'c:\\\\Users\\\\eoinp\\\\anaconda3\\\\envs\\\\deepl\\\\Lib\\\\site-packages\\\\plotly\\\\graph_objs\\\\__init__.py'>\n",
      "\n",
      "File c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\plotly\\express\\_core.py:2103, in make_figure(\n",
      "    args={'animation_frame': None, 'animation_group': None, 'category_orders': None, 'color': 'text_selection_method', 'color_discrete_map': None, 'color_discrete_sequence': None, 'custom_data': None, 'data_frame':       epoch  train_loss  train_accuracy  train_f...3  \n",
      "2024           43  \n",
      "\n",
      "[2025 rows x 35 columns], 'error_x': None, 'error_x_minus': None, ...},\n",
      "    constructor=<class 'plotly.graph_objs._scatter.Scatter'>,\n",
      "    trace_patch={'line': {'shape': None}, 'mode': 'lines+markers'},\n",
      "    layout_patch={}\n",
      ")\n",
      "   2099 trace_specs, grouped_mappings, sizeref, show_colorbar = infer_config(\n",
      "   2100     args, constructor, trace_patch, layout_patch\n",
      "   2101 )\n",
      "   2102 grouper = [x.grouper or one_group for x in grouped_mappings] or [one_group]\n",
      "-> 2103 groups, orders = get_groups_and_orders(args, grouper)\n",
      "        args = {'data_frame':       epoch  train_loss  train_accuracy  train_f1  val_loss  val_accuracy  \\\n",
      "0         1    0.515992        0.507607  0.377019  0.082439      0.477778   \n",
      "1         2    0.501661        0.510373  0.344923  0.085130      0.477778   \n",
      "2         3    0.502206        0.510373  0.344923  0.086253      0.477778   \n",
      "3         4    0.496719        0.510373  0.344923  0.086030      0.477778   \n",
      "4         5    0.496294        0.510373  0.344923  0.086719      0.477778   \n",
      "...     ...         ...             ...       ...       ...           ...   \n",
      "2020      1    0.500901        0.511757  0.387931  0.080100      0.577778   \n",
      "2021      2    0.507910        0.497925  0.331031  0.094164      0.577778   \n",
      "2022      3    0.505884        0.497925  0.331031  0.093335      0.577778   \n",
      "2023      4    0.504886        0.493776  0.343227  0.093252      0.577778   \n",
      "2024      5    0.502507        0.497925  0.331031  0.092409      0.577778   \n",
      "\n",
      "        val_f1             ts_encoder_name  ts_encoder_ts_window  \\\n",
      "0     0.308939  TimeSeriesTransformerModel                    10   \n",
      "1     0.308939  TimeSeriesTransformerModel                    10   \n",
      "2     0.308939  TimeSeriesTransformerModel                    10   \n",
      "3     0.308939  TimeSeriesTransformerModel                    10   \n",
      "4     0.308939  TimeSeriesTransformerModel                    10   \n",
      "...        ...                         ...                   ...   \n",
      "2020  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2021  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2022  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2023  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2024  0.423161  TimeSeriesTransformerModel                    10   \n",
      "\n",
      "      ts_encoder_context_length  ...  text_window text_selection_method  \\\n",
      "0                             1  ...            1            [TFIDF, 5]   \n",
      "1                             1  ...            1            [TFIDF, 5]   \n",
      "2                             1  ...            1            [TFIDF, 5]   \n",
      "3                             1  ...            1            [TFIDF, 5]   \n",
      "4                             1  ...            1            [TFIDF, 5]   \n",
      "...                         ...  ...          ...                   ...   \n",
      "2020                          1  ...            1            [TFIDF, 5]   \n",
      "2021                          1  ...            1            [TFIDF, 5]   \n",
      "2022                          1  ...            1            [TFIDF, 5]   \n",
      "2023                          1  ...            1            [TFIDF, 5]   \n",
      "2024                          1  ...            1            [TFIDF, 5]   \n",
      "\n",
      "      data_source_name                              data_source_text_path  \\\n",
      "0        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "1        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "3        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "4        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "...                ...                                                ...   \n",
      "2020     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2021     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2022     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2023     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2024     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "\n",
      "               data_source_ts_path data_source_ts_date_col  \\\n",
      "0     ./data/stock_emotions/price/                    Date   \n",
      "1     ./data/stock_emotions/price/                    Date   \n",
      "2     ./data/stock_emotions/price/                    Date   \n",
      "3     ./data/stock_emotions/price/                    Date   \n",
      "4     ./data/stock_emotions/price/                    Date   \n",
      "...                            ...                     ...   \n",
      "2020  ./data/stock_emotions/price/                    Date   \n",
      "2021  ./data/stock_emotions/price/                    Date   \n",
      "2022  ./data/stock_emotions/price/                    Date   \n",
      "2023  ./data/stock_emotions/price/                    Date   \n",
      "2024  ./data/stock_emotions/price/                    Date   \n",
      "\n",
      "      data_source_text_date_col  data_source_text_col negatives_creation  \\\n",
      "0                          date                  text        [naive, 60]   \n",
      "1                          date                  text        [naive, 60]   \n",
      "2                          date                  text        [naive, 60]   \n",
      "3                          date                  text        [naive, 60]   \n",
      "4                          date                  text        [naive, 60]   \n",
      "...                         ...                   ...                ...   \n",
      "2020                       date                  text        [naive, 60]   \n",
      "2021                       date                  text        [naive, 60]   \n",
      "2022                       date                  text        [naive, 60]   \n",
      "2023                       date                  text        [naive, 60]   \n",
      "2024                       date                  text        [naive, 60]   \n",
      "\n",
      "     random_state  \n",
      "0              42  \n",
      "1              42  \n",
      "2              42  \n",
      "3              42  \n",
      "4              42  \n",
      "...           ...  \n",
      "2020           43  \n",
      "2021           43  \n",
      "2022           43  \n",
      "2023           43  \n",
      "2024           43  \n",
      "\n",
      "[2025 rows x 35 columns], 'x': 'epoch', 'y': 'train_loss', 'line_group': None, 'color': 'text_selection_method', 'line_dash': None, 'symbol': None, 'hover_name': None, 'hover_data': None, 'custom_data': None, 'text': None, 'facet_row': None, 'facet_col': None, 'facet_col_wrap': 0, 'facet_row_spacing': None, 'facet_col_spacing': None, 'error_x': None, 'error_x_minus': None, 'error_y': None, 'error_y_minus': None, 'animation_frame': None, 'animation_group': None, 'category_orders': None, 'labels': {'epoch': 'Epoch', 'train_loss': 'Train Loss', 'text_selection_method': 'Text selection method'}, 'orientation': None, 'color_discrete_sequence': None, 'color_discrete_map': None, 'line_dash_sequence': None, 'line_dash_map': None, 'symbol_sequence': None, 'symbol_map': None, 'markers': True, 'log_x': False, 'log_y': False, 'range_x': None, 'range_y': None, 'line_shape': None, 'render_mode': 'auto', 'title': 'Train Loss Over Epochs', 'template': None, 'width': None, 'height': None}\n",
      "        grouper = ['text_selection_method', <function one_group at 0x000001F183F8DEE0>, <function one_group at 0x000001F183F8DEE0>, <function one_group at 0x000001F183F8DEE0>, <function one_group at 0x000001F183F8DEE0>, <function one_group at 0x000001F183F8DEE0>, <function one_group at 0x000001F183F8DEE0>]\n",
      "   2105 col_labels = []\n",
      "   2106 row_labels = []\n",
      "\n",
      "File c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\plotly\\express\\_core.py:2037, in get_groups_and_orders(\n",
      "    args={'animation_frame': None, 'animation_group': None, 'category_orders': None, 'color': 'text_selection_method', 'color_discrete_map': None, 'color_discrete_sequence': None, 'custom_data': None, 'data_frame':       epoch  train_loss  train_accuracy  train_f...3  \n",
      "2024           43  \n",
      "\n",
      "[2025 rows x 35 columns], 'error_x': None, 'error_x_minus': None, ...},\n",
      "    grouper=['text_selection_method', <function one_group>, <function one_group>, <function one_group>, <function one_group>, <function one_group>, <function one_group>]\n",
      ")\n",
      "   2035 else:\n",
      "   2036     if col not in unique_cache:\n",
      "-> 2037         unique_cache[col] = list(args[\"data_frame\"][col].unique())\n",
      "        unique_cache = {}\n",
      "        col = 'text_selection_method'\n",
      "        args[\"data_frame\"] =       epoch  train_loss  train_accuracy  train_f1  val_loss  val_accuracy  \\\n",
      "0         1    0.515992        0.507607  0.377019  0.082439      0.477778   \n",
      "1         2    0.501661        0.510373  0.344923  0.085130      0.477778   \n",
      "2         3    0.502206        0.510373  0.344923  0.086253      0.477778   \n",
      "3         4    0.496719        0.510373  0.344923  0.086030      0.477778   \n",
      "4         5    0.496294        0.510373  0.344923  0.086719      0.477778   \n",
      "...     ...         ...             ...       ...       ...           ...   \n",
      "2020      1    0.500901        0.511757  0.387931  0.080100      0.577778   \n",
      "2021      2    0.507910        0.497925  0.331031  0.094164      0.577778   \n",
      "2022      3    0.505884        0.497925  0.331031  0.093335      0.577778   \n",
      "2023      4    0.504886        0.493776  0.343227  0.093252      0.577778   \n",
      "2024      5    0.502507        0.497925  0.331031  0.092409      0.577778   \n",
      "\n",
      "        val_f1             ts_encoder_name  ts_encoder_ts_window  \\\n",
      "0     0.308939  TimeSeriesTransformerModel                    10   \n",
      "1     0.308939  TimeSeriesTransformerModel                    10   \n",
      "2     0.308939  TimeSeriesTransformerModel                    10   \n",
      "3     0.308939  TimeSeriesTransformerModel                    10   \n",
      "4     0.308939  TimeSeriesTransformerModel                    10   \n",
      "...        ...                         ...                   ...   \n",
      "2020  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2021  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2022  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2023  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2024  0.423161  TimeSeriesTransformerModel                    10   \n",
      "\n",
      "      ts_encoder_context_length  ...  text_window text_selection_method  \\\n",
      "0                             1  ...            1            [TFIDF, 5]   \n",
      "1                             1  ...            1            [TFIDF, 5]   \n",
      "2                             1  ...            1            [TFIDF, 5]   \n",
      "3                             1  ...            1            [TFIDF, 5]   \n",
      "4                             1  ...            1            [TFIDF, 5]   \n",
      "...                         ...  ...          ...                   ...   \n",
      "2020                          1  ...            1            [TFIDF, 5]   \n",
      "2021                          1  ...            1            [TFIDF, 5]   \n",
      "2022                          1  ...            1            [TFIDF, 5]   \n",
      "2023                          1  ...            1            [TFIDF, 5]   \n",
      "2024                          1  ...            1            [TFIDF, 5]   \n",
      "\n",
      "      data_source_name                              data_source_text_path  \\\n",
      "0        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "1        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "3        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "4        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "...                ...                                                ...   \n",
      "2020     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2021     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2022     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2023     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2024     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "\n",
      "               data_source_ts_path data_source_ts_date_col  \\\n",
      "0     ./data/stock_emotions/price/                    Date   \n",
      "1     ./data/stock_emotions/price/                    Date   \n",
      "2     ./data/stock_emotions/price/                    Date   \n",
      "3     ./data/stock_emotions/price/                    Date   \n",
      "4     ./data/stock_emotions/price/                    Date   \n",
      "...                            ...                     ...   \n",
      "2020  ./data/stock_emotions/price/                    Date   \n",
      "2021  ./data/stock_emotions/price/                    Date   \n",
      "2022  ./data/stock_emotions/price/                    Date   \n",
      "2023  ./data/stock_emotions/price/                    Date   \n",
      "2024  ./data/stock_emotions/price/                    Date   \n",
      "\n",
      "      data_source_text_date_col  data_source_text_col negatives_creation  \\\n",
      "0                          date                  text        [naive, 60]   \n",
      "1                          date                  text        [naive, 60]   \n",
      "2                          date                  text        [naive, 60]   \n",
      "3                          date                  text        [naive, 60]   \n",
      "4                          date                  text        [naive, 60]   \n",
      "...                         ...                   ...                ...   \n",
      "2020                       date                  text        [naive, 60]   \n",
      "2021                       date                  text        [naive, 60]   \n",
      "2022                       date                  text        [naive, 60]   \n",
      "2023                       date                  text        [naive, 60]   \n",
      "2024                       date                  text        [naive, 60]   \n",
      "\n",
      "     random_state  \n",
      "0              42  \n",
      "1              42  \n",
      "2              42  \n",
      "3              42  \n",
      "4              42  \n",
      "...           ...  \n",
      "2020           43  \n",
      "2021           43  \n",
      "2022           43  \n",
      "2023           43  \n",
      "2024           43  \n",
      "\n",
      "[2025 rows x 35 columns]\n",
      "        args = {'data_frame':       epoch  train_loss  train_accuracy  train_f1  val_loss  val_accuracy  \\\n",
      "0         1    0.515992        0.507607  0.377019  0.082439      0.477778   \n",
      "1         2    0.501661        0.510373  0.344923  0.085130      0.477778   \n",
      "2         3    0.502206        0.510373  0.344923  0.086253      0.477778   \n",
      "3         4    0.496719        0.510373  0.344923  0.086030      0.477778   \n",
      "4         5    0.496294        0.510373  0.344923  0.086719      0.477778   \n",
      "...     ...         ...             ...       ...       ...           ...   \n",
      "2020      1    0.500901        0.511757  0.387931  0.080100      0.577778   \n",
      "2021      2    0.507910        0.497925  0.331031  0.094164      0.577778   \n",
      "2022      3    0.505884        0.497925  0.331031  0.093335      0.577778   \n",
      "2023      4    0.504886        0.493776  0.343227  0.093252      0.577778   \n",
      "2024      5    0.502507        0.497925  0.331031  0.092409      0.577778   \n",
      "\n",
      "        val_f1             ts_encoder_name  ts_encoder_ts_window  \\\n",
      "0     0.308939  TimeSeriesTransformerModel                    10   \n",
      "1     0.308939  TimeSeriesTransformerModel                    10   \n",
      "2     0.308939  TimeSeriesTransformerModel                    10   \n",
      "3     0.308939  TimeSeriesTransformerModel                    10   \n",
      "4     0.308939  TimeSeriesTransformerModel                    10   \n",
      "...        ...                         ...                   ...   \n",
      "2020  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2021  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2022  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2023  0.423161  TimeSeriesTransformerModel                    10   \n",
      "2024  0.423161  TimeSeriesTransformerModel                    10   \n",
      "\n",
      "      ts_encoder_context_length  ...  text_window text_selection_method  \\\n",
      "0                             1  ...            1            [TFIDF, 5]   \n",
      "1                             1  ...            1            [TFIDF, 5]   \n",
      "2                             1  ...            1            [TFIDF, 5]   \n",
      "3                             1  ...            1            [TFIDF, 5]   \n",
      "4                             1  ...            1            [TFIDF, 5]   \n",
      "...                         ...  ...          ...                   ...   \n",
      "2020                          1  ...            1            [TFIDF, 5]   \n",
      "2021                          1  ...            1            [TFIDF, 5]   \n",
      "2022                          1  ...            1            [TFIDF, 5]   \n",
      "2023                          1  ...            1            [TFIDF, 5]   \n",
      "2024                          1  ...            1            [TFIDF, 5]   \n",
      "\n",
      "      data_source_name                              data_source_text_path  \\\n",
      "0        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "1        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "3        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "4        stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "...                ...                                                ...   \n",
      "2020     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2021     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2022     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2023     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "2024     stock_emotion  ./data/stock_emotions/tweet/processed_stockemo...   \n",
      "\n",
      "               data_source_ts_path data_source_ts_date_col  \\\n",
      "0     ./data/stock_emotions/price/                    Date   \n",
      "1     ./data/stock_emotions/price/                    Date   \n",
      "2     ./data/stock_emotions/price/                    Date   \n",
      "3     ./data/stock_emotions/price/                    Date   \n",
      "4     ./data/stock_emotions/price/                    Date   \n",
      "...                            ...                     ...   \n",
      "2020  ./data/stock_emotions/price/                    Date   \n",
      "2021  ./data/stock_emotions/price/                    Date   \n",
      "2022  ./data/stock_emotions/price/                    Date   \n",
      "2023  ./data/stock_emotions/price/                    Date   \n",
      "2024  ./data/stock_emotions/price/                    Date   \n",
      "\n",
      "      data_source_text_date_col  data_source_text_col negatives_creation  \\\n",
      "0                          date                  text        [naive, 60]   \n",
      "1                          date                  text        [naive, 60]   \n",
      "2                          date                  text        [naive, 60]   \n",
      "3                          date                  text        [naive, 60]   \n",
      "4                          date                  text        [naive, 60]   \n",
      "...                         ...                   ...                ...   \n",
      "2020                       date                  text        [naive, 60]   \n",
      "2021                       date                  text        [naive, 60]   \n",
      "2022                       date                  text        [naive, 60]   \n",
      "2023                       date                  text        [naive, 60]   \n",
      "2024                       date                  text        [naive, 60]   \n",
      "\n",
      "     random_state  \n",
      "0              42  \n",
      "1              42  \n",
      "2              42  \n",
      "3              42  \n",
      "4              42  \n",
      "...           ...  \n",
      "2020           43  \n",
      "2021           43  \n",
      "2022           43  \n",
      "2023           43  \n",
      "2024           43  \n",
      "\n",
      "[2025 rows x 35 columns], 'x': 'epoch', 'y': 'train_loss', 'line_group': None, 'color': 'text_selection_method', 'line_dash': None, 'symbol': None, 'hover_name': None, 'hover_data': None, 'custom_data': None, 'text': None, 'facet_row': None, 'facet_col': None, 'facet_col_wrap': 0, 'facet_row_spacing': None, 'facet_col_spacing': None, 'error_x': None, 'error_x_minus': None, 'error_y': None, 'error_y_minus': None, 'animation_frame': None, 'animation_group': None, 'category_orders': None, 'labels': {'epoch': 'Epoch', 'train_loss': 'Train Loss', 'text_selection_method': 'Text selection method'}, 'orientation': None, 'color_discrete_sequence': None, 'color_discrete_map': None, 'line_dash_sequence': None, 'line_dash_map': None, 'symbol_sequence': None, 'symbol_map': None, 'markers': True, 'log_x': False, 'log_y': False, 'range_x': None, 'range_y': None, 'line_shape': None, 'render_mode': 'auto', 'title': 'Train Loss Over Epochs', 'template': None, 'width': None, 'height': None}\n",
      "   2038     uniques = unique_cache[col]\n",
      "   2039     if len(uniques) == 1:\n",
      "\n",
      "File c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\pandas\\core\\series.py:2407, in Series.unique(\n",
      "    self=0       [TFIDF, 5]\n",
      "1       [TFIDF, 5]\n",
      "2       [T...ext_selection_method, Length: 2025, dtype: object\n",
      ")\n",
      "   2344 def unique(self) -> ArrayLike:  # pylint: disable=useless-parent-delegation\n",
      "   2345     \"\"\"\n",
      "   2346     Return unique values of Series object.\n",
      "   2347 \n",
      "   (...)\n",
      "   2405     Categories (3, object): ['a' < 'b' < 'c']\n",
      "   2406     \"\"\"\n",
      "-> 2407     return super().unique()\n",
      "\n",
      "File c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\pandas\\core\\base.py:1025, in IndexOpsMixin.unique(\n",
      "    self=0       [TFIDF, 5]\n",
      "1       [TFIDF, 5]\n",
      "2       [T...ext_selection_method, Length: 2025, dtype: object\n",
      ")\n",
      "   1023     result = values.unique()\n",
      "   1024 else:\n",
      "-> 1025     result = algorithms.unique1d(values)\n",
      "        values = array([list(['TFIDF', 5]), list(['TFIDF', 5]), list(['TFIDF', 5]), ...,\n",
      "       list(['TFIDF', 5]), list(['TFIDF', 5]), list(['TFIDF', 5])],\n",
      "      dtype=object)\n",
      "        algorithms.unique1d = <function unique at 0x000001F182326520>\n",
      "        algorithms = <module 'pandas.core.algorithms' from 'c:\\\\Users\\\\eoinp\\\\anaconda3\\\\envs\\\\deepl\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\algorithms.py'>\n",
      "   1026 return result\n",
      "\n",
      "File c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\pandas\\core\\algorithms.py:401, in unique(\n",
      "    values=array([list(['TFIDF', 5]), list(['TFIDF', 5]), l...F', 5]), list(['TFIDF', 5])],\n",
      "      dtype=object)\n",
      ")\n",
      "    307 def unique(values):\n",
      "    308     \"\"\"\n",
      "    309     Return unique values based on a hash table.\n",
      "    310 \n",
      "   (...)\n",
      "    399     array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\n",
      "    400     \"\"\"\n",
      "--> 401     return unique_with_mask(values)\n",
      "        values = array([list(['TFIDF', 5]), list(['TFIDF', 5]), list(['TFIDF', 5]), ...,\n",
      "       list(['TFIDF', 5]), list(['TFIDF', 5]), list(['TFIDF', 5])],\n",
      "      dtype=object)\n",
      "\n",
      "File c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\pandas\\core\\algorithms.py:440, in unique_with_mask(\n",
      "    values=array([list(['TFIDF', 5]), list(['TFIDF', 5]), l...F', 5]), list(['TFIDF', 5])],\n",
      "      dtype=object),\n",
      "    mask=None\n",
      ")\n",
      "    438 table = hashtable(len(values))\n",
      "    439 if mask is None:\n",
      "--> 440     uniques = table.unique(values)\n",
      "        values = array([list(['TFIDF', 5]), list(['TFIDF', 5]), list(['TFIDF', 5]), ...,\n",
      "       list(['TFIDF', 5]), list(['TFIDF', 5]), list(['TFIDF', 5])],\n",
      "      dtype=object)\n",
      "        table = <pandas._libs.hashtable.PyObjectHashTable object at 0x000001F1867CBB70>\n",
      "    441     uniques = _reconstruct_data(uniques, original.dtype, original)\n",
      "    442     return uniques\n",
      "\n",
      "File pandas\\\\_libs\\\\hashtable_class_helper.pxi:7248, in pandas._libs.hashtable.PyObjectHashTable.unique()\n",
      "\n",
      "File pandas\\\\_libs\\\\hashtable_class_helper.pxi:7195, in pandas._libs.hashtable.PyObjectHashTable._unique()\n",
      "\n",
      "TypeError: unhashable type: 'list'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"./output_plotting.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to flatten nested dictionary (dataset_params and model_params)\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process the JSON data into a DataFrame\n",
    "def process_data(data):\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        # Flatten model_params and dataset_params and combine them\n",
    "        flattened_params = {**flatten_dict(entry[\"model_params\"]), **flatten_dict(entry[\"dataset_params\"])}\n",
    "        \n",
    "        # Add train and validation metrics\n",
    "        train_metrics = entry[\"train_metrics\"]\n",
    "        val_metrics = entry[\"val_metrics\"]\n",
    "\n",
    "        for epoch in range(entry[\"epochs\"]):\n",
    "            row = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_metrics['loss'][epoch],\n",
    "                'train_accuracy': train_metrics['accuracy'][epoch],\n",
    "                'train_f1': train_metrics['f1'][epoch],\n",
    "                'val_loss': val_metrics['loss'][epoch],\n",
    "                'val_accuracy': val_metrics['accuracy'][epoch],\n",
    "                'val_f1': val_metrics['f1'][epoch],\n",
    "            }\n",
    "            row.update(flattened_params)  # Include all flattened parameters in the row\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "df = process_data(data)\n",
    "\n",
    "# Get all unique parameter names (excluding metric columns)\n",
    "pivot_options = [col for col in df.columns if col not in ['epoch', 'train_loss', 'train_accuracy', 'train_f1', 'val_loss', 'val_accuracy', 'val_f1']]\n",
    "\n",
    "# Create a Dash app for interactive plotting\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Model Training Metrics\"),\n",
    "    \n",
    "    # Dropdown for selecting the metric to display\n",
    "    html.Label(\"Choose a Metric:\"),\n",
    "    dcc.Dropdown(\n",
    "        id=\"metric\",\n",
    "        options=[\n",
    "            {'label': 'Loss', 'value': 'loss'},\n",
    "            {'label': 'Accuracy', 'value': 'accuracy'},\n",
    "            {'label': 'F1 Score', 'value': 'f1'},\n",
    "        ],\n",
    "        value='loss'\n",
    "    ),\n",
    "    \n",
    "    # Dropdown for selecting the hyperparameter to pivot over (dynamic based on dataset)\n",
    "    html.Label(\"Choose a Hyperparameter to Pivot Over:\"),\n",
    "    dcc.Dropdown(\n",
    "        id=\"pivot_by\",\n",
    "        options=[{'label': col.replace('_', ' ').capitalize(), 'value': col} for col in pivot_options],\n",
    "        value='learning_rate'\n",
    "    ),\n",
    "\n",
    "    # Toggle to switch between individual performance and summary view\n",
    "    dcc.RadioItems(\n",
    "        id='view_mode',\n",
    "        options=[\n",
    "            {'label': 'Individual', 'value': 'individual'},\n",
    "            {'label': 'Summary', 'value': 'summary'}\n",
    "        ],\n",
    "        value='individual',\n",
    "        labelStyle={'display': 'inline-block'}\n",
    "    ),\n",
    "    \n",
    "    # Plot for training metrics\n",
    "    dcc.Graph(id='train_metric_graph'),\n",
    "    \n",
    "    # Plot for validation metrics\n",
    "    dcc.Graph(id='val_metric_graph')\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('train_metric_graph', 'figure'),\n",
    "     Output('val_metric_graph', 'figure')],\n",
    "    [Input('metric', 'value'),\n",
    "     Input('pivot_by', 'value'),\n",
    "     Input('view_mode', 'value')]\n",
    ")\n",
    "def update_graph(selected_metric, pivot_by, view_mode):\n",
    "    # Columns for train and validation metrics\n",
    "    train_metric_col = f\"train_{selected_metric}\"\n",
    "    val_metric_col = f\"val_{selected_metric}\"\n",
    "\n",
    "    if view_mode == 'summary':\n",
    "        # Calculate summary for each epoch (min, max, avg) for each value of pivot_by\n",
    "        summary_df = df.groupby(['epoch', pivot_by]).agg(\n",
    "            avg_train_metric=(train_metric_col, 'mean'),\n",
    "            min_train_metric=(train_metric_col, 'min'),\n",
    "            max_train_metric=(train_metric_col, 'max'),\n",
    "            avg_val_metric=(val_metric_col, 'mean'),\n",
    "            min_val_metric=(val_metric_col, 'min'),\n",
    "            max_val_metric=(val_metric_col, 'max')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Prepare figure for training metrics\n",
    "        train_fig = go.Figure()\n",
    "\n",
    "        # For each pivot_by group, add average, min, and max lines with the same color\n",
    "        unique_pivot_vals = summary_df[pivot_by].unique()\n",
    "        color_map = px.colors.qualitative.Plotly[:len(unique_pivot_vals)]\n",
    "\n",
    "        for i, val in enumerate(unique_pivot_vals):\n",
    "            pivot_group = summary_df[summary_df[pivot_by] == val]\n",
    "\n",
    "            # Add avg line\n",
    "            train_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['avg_train_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Avg Train ({val})',\n",
    "                line=dict(color=color_map[i], width=2)\n",
    "            ))\n",
    "\n",
    "            # Add min and max as dashed lines\n",
    "            train_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['min_train_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Min Train ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "\n",
    "            train_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['max_train_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Max Train ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "\n",
    "        train_fig.update_layout(\n",
    "            title=f\"Train {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "            xaxis_title=\"Epoch\",\n",
    "            yaxis_title=f\"Train {selected_metric.capitalize()}\"\n",
    "        )\n",
    "\n",
    "        # Prepare figure for validation metrics\n",
    "        val_fig = go.Figure()\n",
    "\n",
    "        for i, val in enumerate(unique_pivot_vals):\n",
    "            pivot_group = summary_df[summary_df[pivot_by] == val]\n",
    "\n",
    "            # Add avg line\n",
    "            val_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['avg_val_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Avg Val ({val})',\n",
    "                line=dict(color=color_map[i], width=2)\n",
    "            ))\n",
    "\n",
    "            # Add min and max as dashed lines\n",
    "            val_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['min_val_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Min Val ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "\n",
    "            val_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['max_val_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Max Val ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "\n",
    "        val_fig.update_layout(\n",
    "            title=f\"Validation {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "            xaxis_title=\"Epoch\",\n",
    "            yaxis_title=f\"Validation {selected_metric.capitalize()}\"\n",
    "        )\n",
    "\n",
    "    else:  # Individual mode\n",
    "        # Plot for training metrics (individual performance)\n",
    "        train_fig = px.line(df, x=\"epoch\", y=train_metric_col, color=pivot_by, markers=True,\n",
    "                            labels={\n",
    "                                \"epoch\": \"Epoch\",\n",
    "                                train_metric_col: f\"Train {selected_metric.capitalize()}\",\n",
    "                                pivot_by: pivot_by.replace(\"_\", \" \").capitalize()\n",
    "                            },\n",
    "                            title=f\"Train {selected_metric.capitalize()} Over Epochs\")\n",
    "        \n",
    "        # Plot for validation metrics (individual performance)\n",
    "        val_fig = px.line(df, x=\"epoch\", y=val_metric_col, color=pivot_by, markers=True,\n",
    "                          labels={\n",
    "                              \"epoch\": \"Epoch\",\n",
    "                              val_metric_col: f\"Validation {selected_metric.capitalize()}\",\n",
    "                              pivot_by: pivot_by.replace(\"_\", \" \").capitalize()\n",
    "                          },\n",
    "                          title=f\"Validation {selected_metric.capitalize()} Over Epochs\")\n",
    "\n",
    "    return train_fig, val_fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px  # For color handling\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"./output_plotting.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to flatten nested dictionary (dataset_params and model_params)\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process the JSON data into a DataFrame\n",
    "def process_data(data):\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        # Flatten model_params and dataset_params and combine them\n",
    "        flattened_params = {**flatten_dict(entry[\"model_params\"]), **flatten_dict(entry[\"dataset_params\"])}\n",
    "        \n",
    "        # Add train and validation metrics\n",
    "        train_metrics = entry[\"train_metrics\"]\n",
    "        val_metrics = entry[\"val_metrics\"]\n",
    "\n",
    "        for epoch in range(entry[\"epochs\"]):\n",
    "            row = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_metrics['loss'][epoch],\n",
    "                'train_accuracy': train_metrics['accuracy'][epoch],\n",
    "                'train_f1': train_metrics['f1'][epoch],\n",
    "                'val_loss': val_metrics['loss'][epoch],\n",
    "                'val_accuracy': val_metrics['accuracy'][epoch],\n",
    "                'val_f1': val_metrics['f1'][epoch],\n",
    "            }\n",
    "            row.update(flattened_params)  # Include all flattened parameters in the row\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "df = process_data(data)\n",
    "\n",
    "# Get all unique parameter names (excluding metric columns)\n",
    "pivot_options = [col for col in df.columns if col not in ['epoch', 'train_loss', 'train_accuracy', 'train_f1', 'val_loss', 'val_accuracy', 'val_f1']]\n",
    "\n",
    "# Create figures for both summary and individual views\n",
    "\n",
    "def create_summary_figures(selected_metric, pivot_by):\n",
    "    # Columns for train and validation metrics\n",
    "    train_metric_col = f\"train_{selected_metric}\"\n",
    "    val_metric_col = f\"val_{selected_metric}\"\n",
    "    \n",
    "    summary_df = df.groupby(['epoch', pivot_by]).agg(\n",
    "        avg_train_metric=(train_metric_col, 'mean'),\n",
    "        min_train_metric=(train_metric_col, 'min'),\n",
    "        max_train_metric=(train_metric_col, 'max'),\n",
    "        avg_val_metric=(val_metric_col, 'mean'),\n",
    "        min_val_metric=(val_metric_col, 'min'),\n",
    "        max_val_metric=(val_metric_col, 'max')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Prepare figure for training metrics\n",
    "    train_fig = go.Figure()\n",
    "\n",
    "    # For each pivot_by group, add average, min, and max lines\n",
    "    unique_pivot_vals = summary_df[pivot_by].unique()\n",
    "    color_map = px.colors.qualitative.Plotly[:len(unique_pivot_vals)]  # Use plotly express color map\n",
    "\n",
    "    for i, val in enumerate(unique_pivot_vals):\n",
    "        pivot_group = summary_df[summary_df[pivot_by] == val]\n",
    "\n",
    "        # Add avg line\n",
    "        train_fig.add_trace(go.Scatter(\n",
    "            x=pivot_group['epoch'],\n",
    "            y=pivot_group['avg_train_metric'],\n",
    "            mode='lines',\n",
    "            name=f'Avg Train ({val})',\n",
    "            line=dict(color=color_map[i], width=2)\n",
    "        ))\n",
    "\n",
    "        # Add min and max as dashed lines\n",
    "        train_fig.add_trace(go.Scatter(\n",
    "            x=pivot_group['epoch'],\n",
    "            y=pivot_group['min_train_metric'],\n",
    "            mode='lines',\n",
    "            name=f'Min Train ({val})',\n",
    "            line=dict(color=color_map[i], dash='dash')\n",
    "        ))\n",
    "\n",
    "        train_fig.add_trace(go.Scatter(\n",
    "            x=pivot_group['epoch'],\n",
    "            y=pivot_group['max_train_metric'],\n",
    "            mode='lines',\n",
    "            name=f'Max Train ({val})',\n",
    "            line=dict(color=color_map[i], dash='dash')\n",
    "        ))\n",
    "\n",
    "    train_fig.update_layout(\n",
    "        title=f\"Train {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=f\"Train {selected_metric.capitalize()}\"\n",
    "    )\n",
    "\n",
    "    # Prepare figure for validation metrics\n",
    "    val_fig = go.Figure()\n",
    "\n",
    "    for i, val in enumerate(unique_pivot_vals):\n",
    "        pivot_group = summary_df[summary_df[pivot_by] == val]\n",
    "\n",
    "        # Add avg line\n",
    "        val_fig.add_trace(go.Scatter(\n",
    "            x=pivot_group['epoch'],\n",
    "            y=pivot_group['avg_val_metric'],\n",
    "            mode='lines',\n",
    "            name=f'Avg Val ({val})',\n",
    "            line=dict(color=color_map[i], width=2)\n",
    "        ))\n",
    "\n",
    "        # Add min and max as dashed lines\n",
    "        val_fig.add_trace(go.Scatter(\n",
    "            x=pivot_group['epoch'],\n",
    "            y=pivot_group['min_val_metric'],\n",
    "            mode='lines',\n",
    "            name=f'Min Val ({val})',\n",
    "            line=dict(color=color_map[i], dash='dash')\n",
    "        ))\n",
    "\n",
    "        val_fig.add_trace(go.Scatter(\n",
    "            x=pivot_group['epoch'],\n",
    "            y=pivot_group['max_val_metric'],\n",
    "            mode='lines',\n",
    "            name=f'Max Val ({val})',\n",
    "            line=dict(color=color_map[i], dash='dash')\n",
    "        ))\n",
    "\n",
    "    val_fig.update_layout(\n",
    "        title=f\"Validation {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=f\"Validation {selected_metric.capitalize()}\"\n",
    "    )\n",
    "\n",
    "    return train_fig, val_fig\n",
    "\n",
    "# Create summary figures for both train and validation data\n",
    "train_fig, val_fig = create_summary_figures('loss', 'learning_rate')\n",
    "\n",
    "# Create a subplot layout to contain both figures\n",
    "combined_fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Train Metrics\", \"Validation Metrics\"))\n",
    "\n",
    "# Add the train and validation figures to the combined figure\n",
    "for trace in train_fig['data']:\n",
    "    combined_fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in val_fig['data']:\n",
    "    combined_fig.add_trace(trace, row=2, col=1)\n",
    "\n",
    "# Update layout of combined figure\n",
    "combined_fig.update_layout(height=800, title_text=\"Training and Validation Metrics Summary\")\n",
    "\n",
    "# Export the combined figure to a single HTML file\n",
    "pio.write_html(combined_fig, file=\"model_metrics_summary.html\", auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"./output_plotting.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to flatten nested dictionary (dataset_params and model_params)\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process the JSON data into a DataFrame\n",
    "def process_data(data):\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        # Flatten model_params and dataset_params and combine them\n",
    "        flattened_params = {**flatten_dict(entry[\"model_params\"]), **flatten_dict(entry[\"dataset_params\"])}\n",
    "        \n",
    "        # Add train and validation metrics\n",
    "        train_metrics = entry[\"train_metrics\"]\n",
    "        val_metrics = entry[\"val_metrics\"]\n",
    "\n",
    "        for epoch in range(entry[\"epochs\"]):\n",
    "            row = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_metrics['loss'][epoch],\n",
    "                'train_accuracy': train_metrics['accuracy'][epoch],\n",
    "                'train_f1': train_metrics['f1'][epoch],\n",
    "                'val_loss': val_metrics['loss'][epoch],\n",
    "                'val_accuracy': val_metrics['accuracy'][epoch],\n",
    "                'val_f1': val_metrics['f1'][epoch],\n",
    "            }\n",
    "            row.update(flattened_params)  # Include all flattened parameters in the row\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "df = process_data(data)\n",
    "\n",
    "# Get all unique parameter names (excluding metric columns)\n",
    "pivot_options = [col for col in df.columns if col not in ['epoch', 'train_loss', 'train_accuracy', 'train_f1', 'val_loss', 'val_accuracy', 'val_f1']]\n",
    "\n",
    "# Create figures for both summary and individual views\n",
    "def create_summary_figures(selected_metric, pivot_by, view_mode):\n",
    "    # Columns for train and validation metrics\n",
    "    train_metric_col = f\"train_{selected_metric}\"\n",
    "    val_metric_col = f\"val_{selected_metric}\"\n",
    "    \n",
    "    summary_df = df.groupby(['epoch', pivot_by]).agg(\n",
    "        avg_train_metric=(train_metric_col, 'mean'),\n",
    "        min_train_metric=(train_metric_col, 'min'),\n",
    "        max_train_metric=(train_metric_col, 'max'),\n",
    "        avg_val_metric=(val_metric_col, 'mean'),\n",
    "        min_val_metric=(val_metric_col, 'min'),\n",
    "        max_val_metric=(val_metric_col, 'max')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Prepare figure for training metrics\n",
    "    train_fig = go.Figure()\n",
    "\n",
    "    # For each pivot_by group, add average, min, and max lines\n",
    "    unique_pivot_vals = summary_df[pivot_by].unique()\n",
    "    color_map = px.colors.qualitative.Plotly[:len(unique_pivot_vals)]\n",
    "\n",
    "    if view_mode == 'summary':\n",
    "        for i, val in enumerate(unique_pivot_vals):\n",
    "            pivot_group = summary_df[summary_df[pivot_by] == val]\n",
    "\n",
    "            # Add avg line\n",
    "            train_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['avg_train_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Avg Train ({val})',\n",
    "                line=dict(color=color_map[i], width=2)\n",
    "            ))\n",
    "\n",
    "            # Add min and max as dashed lines\n",
    "            train_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['min_train_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Min Train ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "\n",
    "            train_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['max_train_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Max Train ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "    else:\n",
    "        # Plot individual performance (for full view)\n",
    "        train_fig = px.line(df, x=\"epoch\", y=train_metric_col, color=pivot_by, markers=True,\n",
    "                            labels={\n",
    "                                \"epoch\": \"Epoch\",\n",
    "                                train_metric_col: f\"Train {selected_metric.capitalize()}\",\n",
    "                                pivot_by: pivot_by.replace(\"_\", \" \").capitalize()\n",
    "                            },\n",
    "                            title=f\"Train {selected_metric.capitalize()} Over Epochs\")\n",
    "\n",
    "    train_fig.update_layout(\n",
    "        title=f\"Train {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=f\"Train {selected_metric.capitalize()}\"\n",
    "    )\n",
    "\n",
    "    # Prepare figure for validation metrics\n",
    "    val_fig = go.Figure()\n",
    "\n",
    "    if view_mode == 'summary':\n",
    "        for i, val in enumerate(unique_pivot_vals):\n",
    "            pivot_group = summary_df[summary_df[pivot_by] == val]\n",
    "\n",
    "            # Add avg line\n",
    "            val_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['avg_val_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Avg Val ({val})',\n",
    "                line=dict(color=color_map[i], width=2)\n",
    "            ))\n",
    "\n",
    "            # Add min and max as dashed lines\n",
    "            val_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['min_val_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Min Val ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "\n",
    "            val_fig.add_trace(go.Scatter(\n",
    "                x=pivot_group['epoch'],\n",
    "                y=pivot_group['max_val_metric'],\n",
    "                mode='lines',\n",
    "                name=f'Max Val ({val})',\n",
    "                line=dict(color=color_map[i], dash='dash')\n",
    "            ))\n",
    "    else:\n",
    "        # Plot individual performance (for full view)\n",
    "        val_fig = px.line(df, x=\"epoch\", y=val_metric_col, color=pivot_by, markers=True,\n",
    "                          labels={\n",
    "                              \"epoch\": \"Epoch\",\n",
    "                              val_metric_col: f\"Validation {selected_metric.capitalize()}\",\n",
    "                              pivot_by: pivot_by.replace(\"_\", \" \").capitalize()\n",
    "                          },\n",
    "                          title=f\"Validation {selected_metric.capitalize()} Over Epochs\")\n",
    "\n",
    "    val_fig.update_layout(\n",
    "        title=f\"Validation {selected_metric.capitalize()} Summary Over Epochs\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=f\"Validation {selected_metric.capitalize()}\"\n",
    "    )\n",
    "\n",
    "    return train_fig, val_fig\n",
    "\n",
    "# Create figures with the chosen metric, pivot, and mode\n",
    "selected_metric = 'loss'\n",
    "pivot_by = 'learning_rate'\n",
    "view_mode = 'summary'\n",
    "train_fig, val_fig = create_summary_figures(selected_metric, pivot_by, view_mode)\n",
    "\n",
    "# Create a subplot layout to contain both figures\n",
    "combined_fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Train Metrics\", \"Validation Metrics\"))\n",
    "\n",
    "# Add the train and validation figures to the combined figure\n",
    "for trace in train_fig['data']:\n",
    "    combined_fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in val_fig['data']:\n",
    "    combined_fig.add_trace(trace, row=2, col=1)\n",
    "\n",
    "# Add dropdowns and buttons\n",
    "combined_fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(label=\"Loss\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"y\": [df['train_loss'], df['val_loss']]}]),\n",
    "                dict(label=\"Accuracy\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"y\": [df['train_accuracy'], df['val_accuracy']]}]),\n",
    "                dict(label=\"F1 Score\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"y\": [df['train_f1'], df['val_f1']]}])\n",
    "            ]),\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.17,\n",
    "            xanchor=\"left\",\n",
    "            y=1.2,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(label=\"Summary View\",\n",
    "                     method=\"relayout\",\n",
    "                     args=[{\"visible\": [True, True]}]),\n",
    "                dict(label=\"Full View\",\n",
    "                     method=\"relayout\",\n",
    "                     args=[{\"visible\": [True, True]}]),\n",
    "            ]),\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.5,\n",
    "            xanchor=\"left\",\n",
    "            y=1.2,\n",
    "            yanchor=\"top\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Export the combined figure to a single HTML file\n",
    "pio.write_html(combined_fig, file=\"model_metrics_summary_with_controls.html\", auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
