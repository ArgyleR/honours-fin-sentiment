{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.path.dirname(os.path.abspath('test_data_helper.py'))\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "import data_helper as dh\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import model_helper as mh\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed, device):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device != 'cpu':        \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "set_seed(42, 'cpu')\n",
    "DATA_SOURCES = [{\n",
    "            \"name\": \"EDT\",\n",
    "            \"text_path\": \"../../data/EDT/evaluate_news.json\",\n",
    "            \"ts_path\": \"../../data/stock_emotions/price/\",\n",
    "            \"ts_date_col\": 'Date',\n",
    "            'text_date_col': 'date',\n",
    "            'text_col': 'text',\n",
    "            'train_dates': '01/01/2020 - 03/09/2020',\n",
    "            'test_dates': '04/09/2020 - 31/12/2020'\n",
    "        },{\n",
    "            \"name\": \"stock_emotion\",\n",
    "            \"text_path\": \"../../data/stock_emotions/tweet/processed_stockemo.csv\",\n",
    "            \"ts_path\": \"../../data/stock_emotions/price/\",\n",
    "            \"ts_date_col\": 'Date',\n",
    "            'text_date_col': 'date',\n",
    "            'text_col': 'text',\n",
    "            'train_dates': '01/01/2020 - 03/09/2020',\n",
    "            'test_dates': '04/09/2020 - 31/12/2020'\n",
    "        },  {\n",
    "            \"name\": \"stock_net\",\n",
    "            \"text_path\": \"../../data/stocknet/tweet/organised_tweet.csv\",\n",
    "            \"ts_path\": \"../../data/stocknet/price/raw/\",\n",
    "            \"ts_date_col\": 'Date',\n",
    "            'text_date_col': 'created_at',\n",
    "            'text_col': 'text',\n",
    "            'train_dates': '01/01/2014 - 01/08/2015',\n",
    "            'test_dates': '01/08/2015 - 01/01/2016'\n",
    "        }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\eoinp\\appdata\\local\\temp\\ipykernel_96572\\4080804378.py\u001b[0m(96)\u001b[0;36mtest_create_pairs\u001b[1;34m()\u001b[0m\n",
      "\n",
      "                                          ids ticker target_date_text_df  \\\n",
      "0                    [152463, 154113, 170317]    BAC          2020-03-17   \n",
      "1                            [127255, 194636]    BAC          2020-03-24   \n",
      "2                            [194636, 189151]    BAC          2020-03-25   \n",
      "3                    [173896, 173064, 138840]    BAC          2020-03-30   \n",
      "4            [173064, 178277, 173896, 121665]    BAC          2020-03-31   \n",
      "..                                        ...    ...                 ...   \n",
      "625      [16637, 1365, 61918, 209744, 252852]    BAC          2021-03-16   \n",
      "626      [270801, 36200, 43963, 55540, 29453]    JNJ          2021-03-23   \n",
      "627  [122279, 174834, 198701, 156672, 136235]    JNJ          2020-08-03   \n",
      "628  [164458, 127782, 126815, 144666, 139022]    JNJ          2020-05-20   \n",
      "629                  [191762, 150819, 145656]    BAC          2020-05-25   \n",
      "\n",
      "       end_date                                        text_series  \\\n",
      "0    2020-03-19  [NEW YORK, March 17, 2020 /PRNewswire/ --The g...   \n",
      "1    2020-03-26  [CULVER CITY, Calif., March 24, 2020 /PRNewswi...   \n",
      "2    2020-03-27  [BALTIMORE, March 27, 2020 /PRNewswire/ --In r...   \n",
      "3    2020-04-01  [NEW YORK, March 30, 2020 /PRNewswire/ --Inves...   \n",
      "4    2020-04-02  [CLEVELAND, April 2, 2020 /PRNewswire/ -- Bank...   \n",
      "..          ...                                                ...   \n",
      "625  2021-03-18  [NEW YORK, March 18, 2021 /PRNewswire/ --Inves...   \n",
      "626  2021-03-25  [LAKELAND, Fla.--(BUSINESS WIRE)--Starting wit...   \n",
      "627  2020-08-05  [TRENTON, N.J., Aug. 5, 2020 /PRNewswire/ --A ...   \n",
      "628  2020-05-22  [NEW YORK, May 22, 2020 /PRNewswire/ -- The gl...   \n",
      "629  2020-05-27  [NEW YORK, May 27, 2020 /PRNewswire/ --Investo...   \n",
      "\n",
      "                                            text_dates  text_id  \\\n",
      "0                 [2020-03-17, 2020-03-19, 2020-03-17]    text1   \n",
      "1                             [2020-03-24, 2020-03-25]    text3   \n",
      "2                             [2020-03-25, 2020-03-27]    text4   \n",
      "3                 [2020-03-31, 2020-04-01, 2020-03-30]    text6   \n",
      "4     [2020-04-01, 2020-04-02, 2020-03-31, 2020-04-02]    text7   \n",
      "..                                                 ...      ...   \n",
      "625  [2021-03-17, 2021-03-17, 2021-03-18, 2021-03-1...  text178   \n",
      "626  [2021-03-25, 2021-03-24, 2021-03-25, 2021-03-2...  text458   \n",
      "627  [2020-08-03, 2020-08-05, 2020-08-04, 2020-08-0...  text303   \n",
      "628  [2020-05-21, 2020-05-20, 2020-05-22, 2020-05-2...  text254   \n",
      "629               [2020-05-27, 2020-05-27, 2020-05-25]   text36   \n",
      "\n",
      "    target_date_ts_df                                        time_series  \\\n",
      "0          2020-03-19  [21.200000762939453, 19.670000076293945, 18.07...   \n",
      "1          2020-03-26  [22.71999931335449, 21.600000381469727, 22.040...   \n",
      "2          2020-03-27  [21.600000381469727, 22.040000915527344, 21.22...   \n",
      "3          2020-04-01  [19.770000457763672, 20.56999969482422, 20.030...   \n",
      "4          2020-04-02  [20.56999969482422, 20.030000686645508, 21.389...   \n",
      "..                ...                                                ...   \n",
      "625        2020-12-10  [210.5200042724609, 213.25999450683597, 214.19...   \n",
      "626        2020-12-11  [213.25999450683597, 214.1999969482422, 214.13...   \n",
      "627        2020-12-16  [219.27999877929688, 219.4199981689453, 218.58...   \n",
      "628        2020-12-17  [219.4199981689453, 218.58999633789065, 222.58...   \n",
      "629        2020-12-18  [218.58999633789065, 222.58999633789065, 223.9...   \n",
      "\n",
      "                                      ts_past_features  ts_id  label  \n",
      "0    [[2020, 3, 19], [2020, 3, 20], [2020, 3, 23], ...   ts55      1  \n",
      "1    [[2020, 3, 26], [2020, 3, 27], [2020, 3, 30], ...   ts60      1  \n",
      "2    [[2020, 3, 27], [2020, 3, 30], [2020, 3, 31], ...   ts61      1  \n",
      "3    [[2020, 4, 1], [2020, 4, 2], [2020, 4, 3], [20...   ts64      1  \n",
      "4    [[2020, 4, 2], [2020, 4, 3], [2020, 4, 6], [20...   ts65      1  \n",
      "..                                                 ...    ...    ...  \n",
      "625  [[2020, 12, 10], [2020, 12, 11], [2020, 12, 14...  ts736     -1  \n",
      "626  [[2020, 12, 11], [2020, 12, 14], [2020, 12, 15...  ts737     -1  \n",
      "627  [[2020, 12, 16], [2020, 12, 17], [2020, 12, 18...  ts740     -1  \n",
      "628  [[2020, 12, 17], [2020, 12, 18], [2020, 12, 21...  ts741     -1  \n",
      "629  [[2020, 12, 18], [2020, 12, 21], [2020, 12, 22...  ts742     -1  \n",
      "\n",
      "[630 rows x 12 columns]\n",
      "Index(['ids', 'ticker', 'target_date_text_df', 'end_date', 'text_series',\n",
      "       'text_dates', 'text_id', 'target_date_ts_df', 'time_series',\n",
      "       'ts_past_features', 'ts_id', 'label'],\n",
      "      dtype='object')\n",
      "ticker\n",
      "MSFT    228\n",
      "JNJ     225\n",
      "BAC     177\n",
      "Name: count, dtype: int64\n",
      "Index(['ids', 'ticker', 'target_date_text_df', 'end_date', 'text_series',\n",
      "       'text_dates', 'text_id', 'target_date_ts_df', 'time_series',\n",
      "       'ts_past_features', 'ts_id', 'label'],\n",
      "      dtype='object')\n",
      "[datetime.date(2020, 3, 17), datetime.date(2020, 3, 19), datetime.date(2020, 3, 17)]\n",
      "datetime.date(2020, 3, 19)\n",
      "datetime.date(2020, 3, 17)\n",
      "datetime.date(2020, 3, 19)\n",
      "[[2020, 3, 19], [2020, 3, 20], [2020, 3, 23], [2020, 3, 24], [2020, 3, 25], [2020, 3, 26]]\n",
      "[datetime.date(2020, 3, 17), datetime.date(2020, 3, 19), datetime.date(2020, 3, 17)]\n"
     ]
    }
   ],
   "source": [
    "def plot_ticker_per_day(df):\n",
    "        ticker_date_count = df.groupby(['ticker', 'date']).size().reset_index(name='count')\n",
    "        # Create bar plots for each ticker\n",
    "        unique_tickers = ticker_date_count['ticker'].unique()\n",
    "        pdb.set_trace()\n",
    "\n",
    "        for ticker in unique_tickers:\n",
    "                ticker_data = ticker_date_count[ticker_date_count['ticker'] == ticker]\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.bar(ticker_data['date'], ticker_data['count'], color='blue')\n",
    "                plt.title(f'Number of rows for {ticker} per day')\n",
    "                plt.xlabel('Date')\n",
    "                plt.ylabel('Number of rows')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "def test_wrangle_data():\n",
    "    for data_source in DATA_SOURCES:\n",
    "        text_df, ts_df = dh.wrangle_data(data_source)\n",
    "        expected_columns_text = {'ticker', 'date', 'text'}\n",
    "        assert set(expected_columns_text).issubset(set(text_df.columns)), f\"The expected columns {expected_columns_text} should exist in the text DataFrame but got {text_df.columns} instead\"\n",
    "        \n",
    "        expected_columns_ts = {'ticker', 'date', 'Close'}\n",
    "        assert set(expected_columns_ts).issubset(set(ts_df.columns)), f\"The expected columns {expected_columns_ts} should exist in the text DataFrame but got {ts_df.columns} instead\"\n",
    "        \n",
    "        #text_df = text_df[text_df['ticker'].isin(top_tickers)].reset_index(drop=True)\n",
    "        ##check the dates are within a reasonable period of each other\n",
    "        #earliest_text_date = text_df['date'].min()\n",
    "        #latest_text_date = text_df['date'].max()\n",
    "        #earliest_ts_date = ts_df['date'].min()\n",
    "        #latest_ts_date = ts_df['date'].max()\n",
    "        #reasonable_period = pd.Timedelta(days=30)\n",
    "#\n",
    "        #assert (earliest_ts_date >= earliest_text_date - reasonable_period) or \n",
    "#\n",
    "        #assert (earliest_ts_date >= earliest_text_date - reasonable_period) and (latest_ts_date <= latest_text_date + reasonable_period), \"The earliest and latest dates in the time series data should be within 30 days of the text data date range\"\n",
    "def is_all_empty_strings(lst):\n",
    "    return all(s == '' for s in lst)\n",
    "     \n",
    "def test_get_data():\n",
    "    text_tokenizer = mh.get_text_encoder()\n",
    "    columns_to_check = ['ticker', 'target_date_text_df', 'end_date', 'text_series', 'text_dates', 'text_id', 'target_date_ts_df', 'time_series', 'ts_past_features', 'ts_id', 'label', 'original_ts_past_features']\n",
    "    for data_source in DATA_SOURCES:\n",
    "        df_set1 = dh.get_data(text_tokenizer=text_tokenizer, data_source=data_source, subset_data=True, loaders=False)\n",
    "        df_set2 = dh.get_data(text_tokenizer=text_tokenizer, data_source=data_source, subset_data=True, loaders=False)\n",
    "        df_set3 = dh.get_data(text_tokenizer=text_tokenizer, data_source=data_source, subset_data=True, loaders=False, random_state=1)\n",
    "        \n",
    "        \n",
    "        true_cols = []\n",
    "        for df1, df2, df3 in zip(df_set1, df_set2, df_set3):\n",
    "            #check there are 3 tickers present\n",
    "            assert df1['ticker'].nunique() == 3\n",
    "            assert df2['ticker'].nunique() == 3\n",
    "            assert df3['ticker'].nunique() == 3\n",
    "        \n",
    "            #check if there are any examples of text_series with no text at all    \n",
    "            assert not df1['text_series'].apply(is_all_empty_strings).any(), \"There exists a text series with entirely empty strings\"\n",
    "            assert not df2['text_series'].apply(is_all_empty_strings).any(), \"There exists a text series with entirely empty strings\"\n",
    "            assert not df3['text_series'].apply(is_all_empty_strings).any(), \"There exists a text series with entirely empty strings\"\n",
    "\n",
    "            #test reproducibility     \n",
    "            for column in columns_to_check:\n",
    "                assert df1[column].equals(df2[column]), 'df reproducibility is hindered, dfs are not created the same with the same seed'\n",
    "                #we can't just assert that df3 and df1 cols are the same col by col; this is because sometimes columns will be the same such as ticker or label\n",
    "                \n",
    "                #but we care that the whole df is diff such as text selected, negative pairs etc\n",
    "                true_cols.append(df3[column].equals(df1[column]))\n",
    "        assert not all(true_cols), 'df reproducibility is hindered, all dfs are the same'\n",
    "\n",
    "def test_create_time_series_df():\n",
    "     for data_source in DATA_SOURCES:\n",
    "        text_df, ts_df = dh.wrangle_data(data_source)\n",
    "\n",
    "        text_df, ts_df = dh.subset_data_helper(data_source=data_source, text_df=text_df, ts_df=ts_df)\n",
    "        text_date_col = data_source['text_date_col']\n",
    "        ts_date_col = data_source[\"ts_date_col\"]\n",
    "        text_col = data_source[\"text_col\"]\n",
    "\n",
    "        text_df[text_date_col] = pd.to_datetime(text_df[text_date_col], utc=True).dt.date\n",
    "        ts_df[ts_date_col] = pd.to_datetime(ts_df[ts_date_col], utc=True).dt.date\n",
    "\n",
    "        k = 6\n",
    "        mode='start'\n",
    "        created_df = dh.create_time_series_df(df=ts_df, k=k, mode=mode)\n",
    "        pdb.set_trace()\n",
    "\n",
    "def test_create_text_series_df():\n",
    "     pass\n",
    "\n",
    "def test_create_pairs():\n",
    "     ts_window = 6\n",
    "     ts_mode='start'\n",
    "     text_window=3\n",
    "     text_selection_method=('TFIDF', 5)\n",
    "     for data_source in DATA_SOURCES:\n",
    "        text_df, ts_df = dh.wrangle_data(data_source)\n",
    "\n",
    "        text_df, ts_df = dh.subset_data_helper(data_source=data_source, text_df=text_df, ts_df=ts_df)\n",
    "        text_date_col = data_source['text_date_col']\n",
    "        ts_date_col = data_source[\"ts_date_col\"]\n",
    "        text_col = data_source[\"text_col\"]\n",
    "\n",
    "        text_df[text_date_col] = pd.to_datetime(text_df[text_date_col], utc=True).dt.date\n",
    "        ts_df[ts_date_col] = pd.to_datetime(ts_df[ts_date_col], utc=True).dt.date\n",
    "        #convert df to id, tickers:[list], start_date, texts:list, time_series:list, past_time_features:[list]\n",
    "        text_df, ts_df = dh.process_windows(text_df=text_df, \n",
    "                                         ts_df=ts_df, \n",
    "                                         ts_window=ts_window,\n",
    "                                         ts_mode=ts_mode, \n",
    "                                         text_window=text_window, \n",
    "                                         text_selection_method=text_selection_method, \n",
    "                                         text_col=text_col, \n",
    "                                         text_time_col=text_date_col, \n",
    "                                         ts_time_col=ts_date_col)\n",
    "        df = dh.create_pairs(text_df=text_df, ts_df=ts_df, negatives_creation=('naive', 60))\n",
    "        #need to assert for each row:\n",
    "                #no text date in the text_dates is outside of the range expected\n",
    "                #based on the range from ts_past_features\n",
    "        pdb.set_trace()\n",
    "\n",
    "#test_wrangle_data()\n",
    "#test_get_data()\n",
    "test_create_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
