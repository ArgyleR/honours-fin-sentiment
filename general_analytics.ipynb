{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Params:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>senti_label</th>\n",
       "      <th>original</th>\n",
       "      <th>text</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$AAPL you better get back to $130 again ü§î</td>\n",
       "      <td>AAPL you better get back to again thinking face</td>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$AAPL now we just wait for power hour üòéüí™üèæ</td>\n",
       "      <td>AAPL now we just wait for power hour smiling f...</td>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$AAPL never thought I'd buy this üí∞</td>\n",
       "      <td>AAPL never thought I d buy this money bag</td>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$AAPL bought my first option ever with Apple f...</td>\n",
       "      <td>AAPL bought my first option ever with Apple fo...</td>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$AAPL Buy low, hold for another split in a cou...</td>\n",
       "      <td>AAPL Buy low hold for another split in a coupl...</td>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50276</th>\n",
       "      <td>50276</td>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>LOW</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$LOW Why isn‚Äôt this trending yet ü§î</td>\n",
       "      <td>LOW Why isn t this trending yet thinking face</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50277</th>\n",
       "      <td>50277</td>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>LOW</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$LOW Show us some strength and push over $170‚ÄºÔ∏è</td>\n",
       "      <td>LOW Show us some strength and push over double...</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50278</th>\n",
       "      <td>50278</td>\n",
       "      <td>2020-11-11</td>\n",
       "      <td>LOW</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$LOW retail investors will keep adding until  ...</td>\n",
       "      <td>LOW retail investors will keep adding until ea...</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50279</th>\n",
       "      <td>50279</td>\n",
       "      <td>2020-12-09</td>\n",
       "      <td>LOW</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$LOW in 10mins Squeeze Zone possible morning s...</td>\n",
       "      <td>LOW in mins Squeeze Zone possible morning spik...</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50280</th>\n",
       "      <td>50280</td>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>LOW</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$LOW glad I got those 1/15/21 $160 after their...</td>\n",
       "      <td>LOW glad I got those after their earnings wink...</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50281 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       date ticker senti_label  \\\n",
       "0          0 2020-08-31   AAPL     Bullish   \n",
       "1          1 2020-08-31   AAPL     Bullish   \n",
       "2          2 2020-08-31   AAPL     Bullish   \n",
       "3          3 2020-08-31   AAPL     Bullish   \n",
       "4          4 2020-08-31   AAPL     Bullish   \n",
       "...      ...        ...    ...         ...   \n",
       "50276  50276 2020-10-08    LOW     Bullish   \n",
       "50277  50277 2020-10-08    LOW     Bullish   \n",
       "50278  50278 2020-11-11    LOW     Bullish   \n",
       "50279  50279 2020-12-09    LOW     Bullish   \n",
       "50280  50280 2020-12-16    LOW     Bullish   \n",
       "\n",
       "                                                original  \\\n",
       "0              $AAPL you better get back to $130 again ü§î   \n",
       "1              $AAPL now we just wait for power hour üòéüí™üèæ   \n",
       "2                     $AAPL never thought I'd buy this üí∞   \n",
       "3      $AAPL bought my first option ever with Apple f...   \n",
       "4      $AAPL Buy low, hold for another split in a cou...   \n",
       "...                                                  ...   \n",
       "50276                 $LOW Why isn‚Äôt this trending yet ü§î   \n",
       "50277    $LOW Show us some strength and push over $170‚ÄºÔ∏è   \n",
       "50278  $LOW retail investors will keep adding until  ...   \n",
       "50279  $LOW in 10mins Squeeze Zone possible morning s...   \n",
       "50280  $LOW glad I got those 1/15/21 $160 after their...   \n",
       "\n",
       "                                                    text  \\\n",
       "0        AAPL you better get back to again thinking face   \n",
       "1      AAPL now we just wait for power hour smiling f...   \n",
       "2              AAPL never thought I d buy this money bag   \n",
       "3      AAPL bought my first option ever with Apple fo...   \n",
       "4      AAPL Buy low hold for another split in a coupl...   \n",
       "...                                                  ...   \n",
       "50276      LOW Why isn t this trending yet thinking face   \n",
       "50277  LOW Show us some strength and push over double...   \n",
       "50278  LOW retail investors will keep adding until ea...   \n",
       "50279  LOW in mins Squeeze Zone possible morning spik...   \n",
       "50280  LOW glad I got those after their earnings wink...   \n",
       "\n",
       "                     industry  \n",
       "0      Information Technology  \n",
       "1      Information Technology  \n",
       "2      Information Technology  \n",
       "3      Information Technology  \n",
       "4      Information Technology  \n",
       "...                       ...  \n",
       "50276  Consumer Discretionary  \n",
       "50277  Consumer Discretionary  \n",
       "50278  Consumer Discretionary  \n",
       "50279  Consumer Discretionary  \n",
       "50280  Consumer Discretionary  \n",
       "\n",
       "[50281 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_helper_v3 as dh3\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import model_helper as mh\n",
    "from transformers import TimeSeriesTransformerModel, TimeSeriesTransformerConfig\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "import pdb\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "SAVE_PATH = './images/'\n",
    "\n",
    "def set_seed(seed, device):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device != 'cpu' and device != None:        \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def plot_original_ts(ts_df, save_file, mode='ticker', date_pairs=None, font_size=40):\n",
    "    ts_df['Date'] = pd.to_datetime(ts_df['Date'])\n",
    "\n",
    "    if mode == 'ticker':\n",
    "        # Plot each ticker with a unique color\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        for ticker in ts_df['ticker'].unique():\n",
    "            ticker_df = ts_df[ts_df['ticker'] == ticker]\n",
    "            if ticker == 'DIS': colour = 'orange'\n",
    "            elif ticker == \"AAPL\": colour = None\n",
    "            plt.plot(ticker_df['Date'], ticker_df['Close'], label=ticker, color=colour, linewidth=14)\n",
    "\n",
    "        #plt.legend(title='Ticker', loc='upper center', bbox_to_anchor=(0.5, 0), ncol=14, \n",
    "        #           bbox_transform=plt.gcf().transFigure)#, fontsize=40)\n",
    "        plt.xlabel('Date', fontsize=font_size)\n",
    "        plt.ylabel('Close Price', fontsize=font_size)\n",
    "        #plt.title('Close Price Over Time by Ticker')#, fontsize=50)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1)) # Set major ticks to daily intervals\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))  # Format date as day-month-year\n",
    "\n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.xticks(fontsize=font_size)  # Larger font for x-axis ticks\n",
    "        plt.yticks(fontsize=font_size)  # Larger font for y-axis ticks\n",
    "        if date_pairs:\n",
    "            print(date_pairs)\n",
    "            ymin, ymax = plt.gca().get_ylim()  # Get the current y-axis limits\n",
    "            for start_date, end_date in date_pairs:\n",
    "                # Highlight the range between start_date and end_date, and span the full y-axis range\n",
    "                plt.axvspan(pd.to_datetime(start_date), pd.to_datetime(end_date), \n",
    "                            color='gray', edgecolor='gray', linestyle='--', linewidth=1, \n",
    "                            ymin=0, ymax=1)  # Ensure the box spans the entire y-axis\n",
    "\n",
    "    elif mode == 'industry':\n",
    "        # Ensure 'industry' column exists in the dataframe\n",
    "        if 'industry' not in ts_df.columns:\n",
    "            raise ValueError(\"Industry mode selected, but 'industry' column is missing in ts_df\")\n",
    "\n",
    "        industries = ts_df['industry'].unique()\n",
    "        industry_colors = plt.cm.get_cmap('tab10', len(industries))  # Assign colors to industries\n",
    "\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        # Plot each ticker, colored by its industry\n",
    "        for idx, industry in enumerate(industries):\n",
    "            industry_df = ts_df[ts_df['industry'] == industry]\n",
    "            for ticker in industry_df['ticker'].unique():\n",
    "                ticker_df = industry_df[industry_df['ticker'] == ticker]\n",
    "                # Label the plot with both industry and ticker\n",
    "                plt.plot(ticker_df['Date'], ticker_df['Close'], label=f'{industry} - {ticker}', color=industry_colors(idx))\n",
    "\n",
    "        plt.legend(title='Industry - Ticker', loc='upper center', bbox_to_anchor=(0.5, 0), ncol=8, \n",
    "                   bbox_transform=plt.gcf().transFigure)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Close Price')\n",
    "        plt.title('Close Price Over Time by Industry and Ticker')\n",
    "        plt.grid(True)\n",
    "\n",
    "    elif mode == 'industry_subplot':\n",
    "        # Ensure 'industry' column exists in the dataframe\n",
    "        if 'industry' not in ts_df.columns:\n",
    "            raise ValueError(\"Industry subplot mode selected, but 'industry' column is missing in ts_df\")\n",
    "\n",
    "        industries = ts_df['industry'].unique()\n",
    "        num_industries = len(industries)\n",
    "        \n",
    "        # Create a subplot for each industry\n",
    "        fig, axes = plt.subplots(num_industries, 1, figsize=(20, 6 * num_industries), sharex=True)\n",
    "\n",
    "        if num_industries == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable even if there's only one subplot\n",
    "\n",
    "        # Plot for each industry in its own subplot\n",
    "        for idx, industry in enumerate(industries):\n",
    "            industry_df = ts_df[ts_df['industry'] == industry]\n",
    "            ax = axes[idx]\n",
    "            tickers = industry_df['ticker'].unique()\n",
    "            ticker_colors = plt.cm.get_cmap('tab10', len(tickers))  # Assign colors to tickers\n",
    "            \n",
    "            for t_idx, ticker in enumerate(tickers):\n",
    "                ticker_df = industry_df[industry_df['ticker'] == ticker]\n",
    "                ax.plot(ticker_df['Date'], ticker_df['Close'], label=ticker, color=ticker_colors(t_idx))\n",
    "            \n",
    "            ax.set_title(f'Close Price Over Time for Industry: {industry}')\n",
    "            ax.set_ylabel('Close Price')\n",
    "            ax.legend(title='Ticker', loc='upper left')\n",
    "            ax.grid(True)\n",
    "        \n",
    "        plt.xlabel('Date')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a local file\n",
    "    time.sleep(2)\n",
    "    plt.savefig(save_file + \".png\")\n",
    "\n",
    "\n",
    "def add_industry_labels(df, dataset_name):\n",
    "     if dataset_name == 'stock_emotion':\n",
    "        industry_df = pd.read_csv('./data/stock_emotions/tweet/processed_stockemo.csv')\n",
    "        \n",
    "        # Drop duplicates to ensure a unique mapping between tickers and industries\n",
    "        df_unique = industry_df[['ticker', 'industry']].drop_duplicates()\n",
    "    \n",
    "        # Create a dictionary mapping ticker to industry\n",
    "        ticker_to_industry = dict(zip(df_unique['ticker'], df_unique['industry']))\n",
    "     elif dataset_name == 'stock_net':\n",
    "        stock_df = pd.read_csv('./data/stocknet/StockTable.txt', sep='\\t')\n",
    "        stock_df['Symbol'] = stock_df['Symbol'].str.replace('$', '', regex=False)\n",
    "        # Create a dictionary mapping ticker (Symbol) to industry (Sector)\n",
    "        ticker_to_industry = dict(zip(stock_df['Symbol'], stock_df['Sector']))\n",
    "        \n",
    "        \n",
    "     elif dataset_name == 'EDT':\n",
    "          raise NotImplementedError\n",
    "     df['industry'] = df['ticker'].map(ticker_to_industry)\n",
    "\n",
    "     df['industry'] = df['industry'].fillna('no industry given')\n",
    "     return df\n",
    "\n",
    "def plot_list_as_hist(list_freqs, xlabel, title, colour):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.hist(list_freqs, bins=range(min(list_freqs), max(list_freqs) + 2), edgecolor=colour, color = colour)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    #plt.xticks(rotation=45, ha='right')\n",
    "    # Save the histogram to a file\n",
    "    title = title.replace(\"\\n\", \" \")\n",
    "    title = title.replace(\":\", \" \")\n",
    "    save_name = f'{title}_histogram.png'\n",
    "    plt.savefig(SAVE_PATH + save_name)\n",
    "\n",
    "def plot_text_counts(counts, xlabel, title, colour):\n",
    "    \n",
    "    # Count occurrences of each number of texts\n",
    "    count_values = counts.value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    count_values.plot(kind='bar', color=colour, edgecolor='black')\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    title = title.replace(\"\\n\", \" \")\n",
    "    title = title.replace(\":\", \" \")\n",
    "\n",
    "    save_name = f'{title}_column_graph.png'\n",
    "    plt.savefig(SAVE_PATH + save_name)\n",
    "\n",
    "def plot_text_examples_per_ticker(df, title, colour):\n",
    "    # Count the number of text examples per ticker\n",
    "    text_counts = df['ticker'].value_counts()\n",
    "    \n",
    "    # Plot as a column graph (bar plot)\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    text_counts.plot(kind='bar', color=colour, edgecolor=colour)\n",
    "\n",
    "    plt.xlabel('Ticker')\n",
    "    plt.ylabel('Number of Text Examples')\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='y')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=7)\n",
    "    # Save the plot to a local file\n",
    "    plt.savefig(f'{SAVE_PATH}{title}.png')\n",
    "\n",
    "dataset_param_grid = {                                                                            \n",
    "        \"ts_window\": [4],#6 & 7 had a random error out                                                                         \n",
    "        \"ts_overlap\": ['start'],                                                                    \n",
    "        \"text_window\": [1],#, 2, 3, 4], #4                                                                \n",
    "        'text_selection_method': [('TFIDF', 5)],\n",
    "        \"data_source\": [#{\n",
    "        #    \"name\": \"EDT\",\n",
    "        #    \"text_path\": \"./data/EDT/evaluate_news.json\",\n",
    "        #    \"ts_path\": \"./data/EDT/evaluate_news.json\",\n",
    "        #    \"ts_date_col\": 'Date',\n",
    "        #    'text_date_col': 'date',\n",
    "        #    'text_col': 'text',\n",
    "        #    'train_dates': '01/01/2020 - 03/09/2020',\n",
    "        #    'test_dates': '04/09/2020 - 31/12/2020',\n",
    "        #    'plot_colour': 'darkorchid'\n",
    "        #}#,\n",
    "        {\n",
    "            \"name\": \"stock_emotion\",\n",
    "            \"text_path\": \"./data/stock_emotions/tweet/processed_stockemo.csv\",\n",
    "            \"ts_path\": \"./data/stock_emotions/price/\",\n",
    "            \"ts_date_col\": 'Date',\n",
    "            'text_date_col': 'date',\n",
    "            'text_col': 'text',\n",
    "            'plot_colour': 'deepskyblue',\n",
    "            'train_dates': '01/01/2020 - 03/09/2020',\n",
    "            'test_dates': '04/09/2020 - 31/12/2020'\n",
    "        }#,{\n",
    "        #    \"name\": \"stock_net\",\n",
    "        #    \"text_path\": \"./data/stocknet/tweet/organised_tweet.csv\",\n",
    "        #    \"ts_path\": \"./data/stocknet/price/raw/\",\n",
    "        #    \"ts_date_col\": 'Date',\n",
    "        #    'text_date_col': 'created_at',\n",
    "        #    'text_col': 'text',\n",
    "        #    'plot_colour': 'mediumseagreen',\n",
    "        #    'train_dates': '01/01/2014 - 01/08/2015',\n",
    "        #    'test_dates': '01/08/2015 - 01/01/2016'\n",
    "        #}\n",
    "        ],                                                            \n",
    "        \"negatives_creation\": [(\"naive\", 60)],                          \n",
    "        \"random_state\": [42],\n",
    "    }\n",
    "\n",
    "dataset_permutations = list(itertools.product(*dataset_param_grid.values()))\n",
    "dataset_combinations = [dict(zip(dataset_param_grid.keys(), perm)) for perm in dataset_permutations]\n",
    "for dataset_params in tqdm(dataset_combinations, desc='Dataset Params', position=0):        \n",
    "        #====================================================\n",
    "        #dataset params\n",
    "        #====================================================\n",
    "        ts_window                   = dataset_params['ts_window']\n",
    "        ts_overlap                  = dataset_params['ts_overlap']\n",
    "        text_window                 = dataset_params[\"text_window\"]\n",
    "        text_selection_method       = dataset_params['text_selection_method']\n",
    "        data_source                 = dataset_params[\"data_source\"]\n",
    "        negatives_creation          = dataset_params['negatives_creation']\n",
    "        random_state                = dataset_params[\"random_state\"] #effects everything but first call is the ds creation\n",
    "        plot_colour                 = data_source['plot_colour']\n",
    "        set_seed(random_state, device=None)\n",
    "\n",
    "        text_df, ts_df = dh3.wrangle_data(data_source)\n",
    "        ts_df = dh3.normalize_ts(ts_df)\n",
    "        \n",
    "        df = dh3.get_data(text_tokenizer=None, \n",
    "            data_source=data_source, \n",
    "            ts_window=ts_window, \n",
    "            ts_mode=ts_overlap, \n",
    "            text_window=text_window, \n",
    "            text_selection_method=text_selection_method, \n",
    "            negatives_creation=negatives_creation, \n",
    "            batch_size=None, \n",
    "            num_workers=None, \n",
    "            loaders=False,\n",
    "            subset_data=True)\n",
    "        \n",
    "        break\n",
    "        \n",
    "        #ts_df = add_industry_labels(df=ts_df, dataset_name=data_source['name'])\n",
    "        #text_df = add_industry_labels(df=text_df, dataset_name=data_source['name'])\n",
    "        #df = add_industry_labels(df=df, dataset_name=data_source['name'])\n",
    "\n",
    "        #for each dataset permutation get:\n",
    "        #base dataset:\n",
    "            #1. lenghts of individual texts characters/tokens\n",
    "        #tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #lengths_of_original_texts = text_df[data_source['text_col']].apply(len)\n",
    "        #lengths_of_tokenized_texts = text_df[data_source['text_col']].apply(lambda text: len(tokenizer.tokenize(text)))\n",
    "        #plot_list_as_hist(lengths_of_original_texts, xlabel=\"Length of Original Text (Characters)\", title=f\"Histogram of Character Lengths of Texts in Original {data_source['name']} Dataset\",\n",
    "        #                  colour=plot_colour)\n",
    "        #plot_list_as_hist(lengths_of_tokenized_texts, xlabel=\"Length of Original Text (Tokens)\", title=f\"Histogram of Number of Tokens in Original {data_source['name']} Dataset\",\n",
    "        #                  colour=plot_colour)\n",
    "        #    #2. words associated with a ticker \n",
    "        #    #3. time series differences in distribution / rises and falls\n",
    "        #plot_text_examples_per_ticker(text_df, title=f\"Number of Texts Per Ticker {data_source['name']}\", colour=plot_colour)\n",
    "#       \n",
    "        #ts_df = ts_df[ts_df['ticker'].isin([\"DIS\"])]#, \"AAPL\"])]\n",
    "\n",
    "        #late dates\n",
    "        start_date = '2020-08-03'\n",
    "        end_date = '2020-08-10'\n",
    "\n",
    "        #early dates\n",
    "        start_date = '2020-03-10'\n",
    "        end_date = '2020-03-17'\n",
    "        date_pairs = None\n",
    "        #date_pairs = [(start_date, end_date)]\n",
    "\n",
    "        # Filter the DataFrame to include only rows where 'Date' is between the start and end date\n",
    "        #ts_df = ts_df.loc[(ts_df['Date'] >= start_date) & (ts_df['Date'] <= end_date)]\n",
    "        #print(ts_df)\n",
    "        ##TODO colour by industry label!!\n",
    "        #plot_original_ts(ts_df=ts_df, save_file=SAVE_PATH + data_source['name'] + \"_original_ts_graph_ticker_dis_and_appl\", mode='ticker', date_pairs=date_pairs)\n",
    "        #plot_original_ts(ts_df=ts_df, save_file=SAVE_PATH + data_source['name'] + \"_original_ts_graph_industry\", mode='industry')\n",
    "        #plot_original_ts(ts_df=ts_df, save_file=SAVE_PATH + data_source['name'] + \"_original_ts_graph_industry_subplot\", mode='industry_subplot')\n",
    "        \n",
    "        #\n",
    "        ##processed data:\n",
    "        #    #1. length of texts selected\n",
    "#\n",
    "        #lengths_of_selected_texts = [len(text) for sublist in df['text_series'] for text in sublist]\n",
    "        #lengths_of_tokenized_selected_texts = [len(tokenizer.tokenize(text)) for sublist in df['text_series'] for text in sublist]\n",
    "        #plot_list_as_hist(lengths_of_selected_texts, \n",
    "        #                  xlabel=\"Length of Selected Text (Characters)\", \n",
    "        #                  title=f\"Histogram of Character Lengths of Texts Selected Per Pair from {data_source['name']} Dataset\\nParameters: Text Selection {text_selection_method}, Text Window {text_window}, Random State {random_state}\",\n",
    "        #                  colour=plot_colour)\n",
    "        #plot_list_as_hist(lengths_of_tokenized_selected_texts, \n",
    "        #                  xlabel=\"Length of Selected Text (Tokens)\", \n",
    "        #                  title=f\"Histogram of Number of Tokens in Selected Pairs {data_source['name']} Dataset\\nParameters: Text Selection {text_selection_method}, Text Window {text_window}, Random State {random_state}\",\n",
    "        #                  colour=plot_colour)\n",
    "        #    #2. number of texts found (some are substituted with empty strings)\n",
    "        #number_of_texts_per_example = df['text_series'].apply(lambda x: len([text for text in x if text != \"\"]))\n",
    "        #plot_text_counts(number_of_texts_per_example, \n",
    "        #                  xlabel='Number of Texts Selected Per Pair', \n",
    "        #                  title=f\"Number of Texts Selected Per Pair from {data_source['name']} Dataset\\nParameters: Text Selection {text_selection_method}, Text Window {text_window}, Random State {random_state}\",\n",
    "        #                  colour=plot_colour)\n",
    "        #\n",
    "        #    #3. base embeddings similarities and distances between pos / neg pairs\n",
    "        #    #3. base embedding distributions \n",
    "#\n",
    "        #\n",
    "            \n",
    "text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>72.482498</td>\n",
       "      <td>73.419998</td>\n",
       "      <td>72.379997</td>\n",
       "      <td>73.412498</td>\n",
       "      <td>71.520821</td>\n",
       "      <td>100805600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>74.059998</td>\n",
       "      <td>75.150002</td>\n",
       "      <td>73.797501</td>\n",
       "      <td>75.087502</td>\n",
       "      <td>73.152649</td>\n",
       "      <td>135480400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>74.287498</td>\n",
       "      <td>75.144997</td>\n",
       "      <td>74.125000</td>\n",
       "      <td>74.357498</td>\n",
       "      <td>72.441460</td>\n",
       "      <td>146322800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>73.447502</td>\n",
       "      <td>74.989998</td>\n",
       "      <td>73.187500</td>\n",
       "      <td>74.949997</td>\n",
       "      <td>73.018677</td>\n",
       "      <td>118387200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>74.959999</td>\n",
       "      <td>75.224998</td>\n",
       "      <td>74.370003</td>\n",
       "      <td>74.597504</td>\n",
       "      <td>72.675278</td>\n",
       "      <td>108872000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10170</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>3694.030029</td>\n",
       "      <td>3703.820068</td>\n",
       "      <td>3689.320068</td>\n",
       "      <td>3703.060059</td>\n",
       "      <td>3703.060059</td>\n",
       "      <td>1883780000</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2020-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10171</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>3723.030029</td>\n",
       "      <td>3740.510010</td>\n",
       "      <td>3723.030029</td>\n",
       "      <td>3735.360107</td>\n",
       "      <td>3735.360107</td>\n",
       "      <td>3535460000</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2020-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10172</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>3750.010010</td>\n",
       "      <td>3756.120117</td>\n",
       "      <td>3723.310059</td>\n",
       "      <td>3727.040039</td>\n",
       "      <td>3727.040039</td>\n",
       "      <td>3393290000</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2020-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10173</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>3736.189941</td>\n",
       "      <td>3744.629883</td>\n",
       "      <td>3730.209961</td>\n",
       "      <td>3732.040039</td>\n",
       "      <td>3732.040039</td>\n",
       "      <td>3154850000</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10174</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>3733.270020</td>\n",
       "      <td>3760.199951</td>\n",
       "      <td>3726.879883</td>\n",
       "      <td>3756.070068</td>\n",
       "      <td>3756.070068</td>\n",
       "      <td>3179040000</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10175 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date         Open         High          Low        Close  \\\n",
       "0      2019-12-31    72.482498    73.419998    72.379997    73.412498   \n",
       "1      2020-01-02    74.059998    75.150002    73.797501    75.087502   \n",
       "2      2020-01-03    74.287498    75.144997    74.125000    74.357498   \n",
       "3      2020-01-06    73.447502    74.989998    73.187500    74.949997   \n",
       "4      2020-01-07    74.959999    75.224998    74.370003    74.597504   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "10170  2020-12-24  3694.030029  3703.820068  3689.320068  3703.060059   \n",
       "10171  2020-12-28  3723.030029  3740.510010  3723.030029  3735.360107   \n",
       "10172  2020-12-29  3750.010010  3756.120117  3723.310059  3727.040039   \n",
       "10173  2020-12-30  3736.189941  3744.629883  3730.209961  3732.040039   \n",
       "10174  2020-12-31  3733.270020  3760.199951  3726.879883  3756.070068   \n",
       "\n",
       "         Adj Close      Volume ticker       date  \n",
       "0        71.520821   100805600   AAPL 2019-12-31  \n",
       "1        73.152649   135480400   AAPL 2020-01-02  \n",
       "2        72.441460   146322800   AAPL 2020-01-03  \n",
       "3        73.018677   118387200   AAPL 2020-01-06  \n",
       "4        72.675278   108872000   AAPL 2020-01-07  \n",
       "...            ...         ...    ...        ...  \n",
       "10170  3703.060059  1883780000  ^GSPC 2020-12-24  \n",
       "10171  3735.360107  3535460000  ^GSPC 2020-12-28  \n",
       "10172  3727.040039  3393290000  ^GSPC 2020-12-29  \n",
       "10173  3732.040039  3154850000  ^GSPC 2020-12-30  \n",
       "10174  3756.070068  3179040000  ^GSPC 2020-12-31  \n",
       "\n",
       "[10175 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eoinp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 500])\n",
      "torch.Size([2, 500])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import TimeSeriesTransformerModel, TimeSeriesTransformerConfig\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(\" \".join(list1).split())\n",
    "    set2 = set(\" \".join(list2).split())\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "i = 0\n",
    "\n",
    "def plot_floats_as_hist(float_list, xlabel, title, colour):\n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(float_list, bins=10, edgecolor=colour)  # You can adjust 'bins' as needed\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def handle_tokenisation(time_series, text_series, past_time_features, tokenizer, label):\n",
    "        #helper method reflecting the custom dataset class' input processing\n",
    "        ts_data = [{\n",
    "            \"past_time_values\": torch.tensor(time_series, dtype=torch.float32),\n",
    "            \"past_observed_mask\": torch.ones(1, len(time_series), dtype=torch.long),\n",
    "            \"past_time_features\": torch.tensor([past_time_features], dtype=torch.float32)\n",
    "        }]\n",
    "\n",
    "        #loop over all text and tokenize\n",
    "        text_data = [tokenizer.text_tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length') for text in text_series]\n",
    "        \n",
    "        input_ids = torch.stack([item['input_ids'].squeeze(0) for item in text_data])  # Shape: [number_of_texts, length_of_text]\n",
    "        attention_mask = torch.stack([item['attention_mask'].squeeze(0) for item in text_data])  # Shape: [number_of_texts, length_of_text]\n",
    "        text_data = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return ts_data, text_data, label\n",
    "\n",
    "def get_similarity_metrics(df, model, tokenizer, device):\n",
    "    model.to(device)\n",
    "    jaccard_similarities = []\n",
    "    df = dh3.correct_negative_labels(df, negative_label=-1)\n",
    "    i = 0\n",
    "    for unique_ts_id in df['ts_id'].unique().tolist():\n",
    "        \n",
    "        #print(df[df[\"ts_id\"] == unique_ts_id])\n",
    "        positive_text = df[(df[\"ts_id\"] == unique_ts_id) & (df[\"label\"] == 1)]\n",
    "        negative_text = df[(df[\"ts_id\"] == unique_ts_id) & (df[\"label\"] == -1)]\n",
    "        \n",
    "        #get jaccard similarity\n",
    "        jaccard_similarity_score = jaccard_similarity(positive_text['text_series'].tolist()[0], negative_text['text_series'].tolist()[0])\n",
    "        jaccard_similarities.append(jaccard_similarity_score)\n",
    "        #get embedding similarity between pos and neg text pairs\n",
    "        filtered_df = df[(df[\"ts_id\"] == unique_ts_id)]\n",
    "\n",
    "        #get embedding similarity between positive text / ts and negative text / ts\n",
    "        data_loader = DataLoader(dh3.CustomDataset(df=filtered_df, text_tokenizer=tokenizer, ts_col='time_series', text_col='text_series', label_col='label'), batch_size=2, shuffle=False)\n",
    "        \n",
    "        for ts_data, text_data, labels in data_loader:\n",
    "            ts_data = {\n",
    "                \"past_time_values\": torch.stack([d['past_time_values'].squeeze(1) for d in ts_data], dim=0).to(device),\n",
    "                \"past_observed_mask\": torch.stack([d['past_observed_mask'].squeeze(0) for d in ts_data], dim=0).to(device),\n",
    "                \"past_time_features\": torch.stack([d['past_time_features'].squeeze(0) for d in ts_data], dim=0).to(device)\n",
    "            }\n",
    "            text_data['input_ids'] = text_data['input_ids'].to(device)\n",
    "            text_data['attention_mask'] = text_data['attention_mask'].to(device)\n",
    "            \n",
    "            ts_embeddings, text_embeddings = model(ts_data, text_data)\n",
    "            similarity = cosine_similarity(ts_embeddings, text_embeddings)\n",
    "        #positive_ts_data, positive_text_data, positive_label = handle_tokenisation(time_series=positive_text['time_series'], \n",
    "        #                                                                           text_series=positive_text['text_series'],\n",
    "        #                                                                           past_time_features=positive_text['past_time_features'], \n",
    "        #                                                                           tokenizer=tokenizer, \n",
    "        #                                                                           label=positive_text['label'])\n",
    "        #negative_ts_data, negative_text_data, negative_label = handle_tokenisation(time_series=negative_text['time_series'], \n",
    "        #                                                                           text_series=negative_text['text_series'],\n",
    "        #                                                                           past_time_features=negative_text['past_time_features'], \n",
    "        #                                                                           tokenizer=tokenizer, \n",
    "        #                                                                           label=negative_text['label'])\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        i += 1\n",
    "        if i > 0:\n",
    "            break\n",
    "    plot_floats_as_hist(float_list=jaccard_similarities, xlabel=\"Jaccard Similarity Scores between Positive and Negative Text Pairs (Same Time Series)\", title=\"Histogram of Jaccard Similarity Scores between Positive and Negative Text Pairs (Same Time Series)\", colour=\"blue\")\n",
    "\n",
    "model_param_grid = {\n",
    "            \"ts_encoder\": [{\"name\": 'TimeSeriesTransformerModel'}],#{\"name\": 'AutoFormerModel'}, {\"name\": \"InformerModel\"}],\n",
    "            \"text_encoder\": [{\"name\": 'bert-base-uncased'}, {\"name\": 'bert-base-cased'}],                                                                     \n",
    "            \"text_aggregation_method\": [\"mean\", 'max'],                                                    \n",
    "            \"projection_dim\": [500],  \n",
    "        }\n",
    "\n",
    "ts_encoder                  = model_param_grid[\"ts_encoder\"][0]\n",
    "ts_encoder['ts_window']     = 4\n",
    "ts_encoder['context_length'] = 1\n",
    "ts_encoder['prediction_length']= 0\n",
    "ts_encoder['lags_sequence'] = [i + 1 for i in range(ts_window - 1)]\n",
    "ts_encoder['num_features']  = 3\n",
    "projection_dim = model_param_grid['projection_dim'][0]\n",
    "text_encoder                = model_param_grid[\"text_encoder\"][0]\n",
    "text_aggregation = model_param_grid['text_aggregation_method'][0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_encoder['name'])\n",
    "model = mh.get_model(ts_encoder_config=ts_encoder, text_encoder_config=text_encoder, projection_dim=projection_dim, ts_window=ts_window, text_aggregation=text_aggregation)\n",
    "get_similarity_metrics(df=df, model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set from 2019-12-31 00:00:00 to 2020-03-13 00:00:00\n",
      "Test set from 2020-03-16 00:00:00 to 2020-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eoinp\\AppData\\Local\\Temp\\ipykernel_31316\\1083539468.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['weekday'] = df['Date'].dt.weekday  # Day of the week (0=Monday, ..., 6=Sunday)\n",
      "C:\\Users\\eoinp\\AppData\\Local\\Temp\\ipykernel_31316\\1083539468.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['weekday'] = df['Date'].dt.weekday  # Day of the week (0=Monday, ..., 6=Sunday)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 106\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Inference (Testing)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(model, test_loader):\n",
      "Cell \u001b[1;32mIn[6], line 92\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, epochs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, time_features, observed_mask \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     91\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 92\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add time features\u001b[39;49;00m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Add observed mask\u001b[39;49;00m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     98\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs[:, \u001b[38;5;241m-\u001b[39mprediction_length:], y)\n\u001b[0;32m     99\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1378\u001b[0m, in \u001b[0;36mTimeSeriesTransformerModel.forward\u001b[1;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[0;32m   1375\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   1376\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1378\u001b[0m transformer_inputs, loc, scale, static_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_network_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1389\u001b[0m     enc_input \u001b[38;5;241m=\u001b[39m transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1269\u001b[0m, in \u001b[0;36mTimeSeriesTransformerModel.create_network_inputs\u001b[1;34m(self, past_values, past_time_features, static_categorical_features, static_real_features, past_observed_mask, future_values, future_time_features)\u001b[0m\n\u001b[0;32m   1267\u001b[0m context \u001b[38;5;241m=\u001b[39m past_values[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length :]\n\u001b[0;32m   1268\u001b[0m observed_context \u001b[38;5;241m=\u001b[39m past_observed_mask[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length :]\n\u001b[1;32m-> 1269\u001b[0m _, loc, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1272\u001b[0m     (torch\u001b[38;5;241m.\u001b[39mcat((past_values, future_values), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m future_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (past_values \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m   1275\u001b[0m )\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# static features\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:145\u001b[0m, in \u001b[0;36mTimeSeriesMeanScaler.forward\u001b[1;34m(self, data, observed_indicator)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, observed_indicator: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[0;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m        data (`torch.Tensor` of shape `(batch_size, sequence_length, num_input_channels)`):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m            `(batch_size, 1, num_input_channels)`)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     ts_sum \u001b[38;5;241m=\u001b[39m (\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobserved_indicator\u001b[49m)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    146\u001b[0m     num_observed \u001b[38;5;241m=\u001b[39m observed_indicator\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    148\u001b[0m     scale \u001b[38;5;241m=\u001b[39m ts_sum \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(num_observed, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TimeSeriesTransformerModel, TimeSeriesTransformerConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Create time features (e.g., weekday)\n",
    "def generate_time_features(df):\n",
    "    df['weekday'] = df['Date'].dt.weekday  # Day of the week (0=Monday, ..., 6=Sunday)\n",
    "    time_features = df[['weekday']].values\n",
    "    return torch.tensor(time_features, dtype=torch.float32)\n",
    "\n",
    "# Create observed mask (all ones since we assume no missing data)\n",
    "def generate_observed_mask(df):\n",
    "    observed_mask = torch.ones((len(df), context_length), dtype=torch.float32)\n",
    "    return observed_mask\n",
    "df = ts_df\n",
    "\n",
    "# Parameters\n",
    "context_length = 5  # Number of past days to consider\n",
    "prediction_length = 1  # Predict the next day's value\n",
    "is_multivariate = True  # Set to False for univariate\n",
    "\n",
    "# Prepare dataset\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, df, context_length, prediction_length, is_multivariate=True):\n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.is_multivariate = is_multivariate\n",
    "        \n",
    "        if self.is_multivariate:\n",
    "            self.features = df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].values\n",
    "        else:\n",
    "            self.features = df[['Close']].values\n",
    "        \n",
    "        self.targets = df['Close'].values\n",
    "        self.time_features = generate_time_features(df)\n",
    "        self.observed_mask = generate_observed_mask(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.features) - self.context_length - self.prediction_length + 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx:idx+self.context_length]\n",
    "        y = self.targets[idx+self.context_length:idx+self.context_length+self.prediction_length]\n",
    "        time_features = self.time_features[idx:idx+self.context_length]\n",
    "        observed_mask = self.observed_mask[idx:idx+self.context_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), time_features, observed_mask\n",
    "\n",
    "\n",
    "# Assuming df is your original dataframe\n",
    "df['Date'] = pd.to_datetime(df['Date'])  # Ensure Date is in datetime format\n",
    "df = df.sort_values('Date')  # Sort by Date to maintain the time sequence\n",
    "\n",
    "# Define the split point based on the timestamp\n",
    "split_date = '2020-03-15'  # This is just an example, adjust based on your data\n",
    "\n",
    "# Split into train and test dataframes based on the split date\n",
    "train_df = df[df['Date'] < split_date]\n",
    "test_df = df[df['Date'] >= split_date]\n",
    "\n",
    "print(f\"Training set from {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "print(f\"Test set from {test_df['Date'].min()} to {test_df['Date'].max()}\")\n",
    "\n",
    "# Example of setting up DataLoader\n",
    "train_dataset = StockDataset(train_df, context_length, prediction_length, is_multivariate)\n",
    "test_dataset = StockDataset(test_df, context_length, prediction_length, is_multivariate)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    num_time_features=train_dataset[0][0].shape[1]  # Number of features per timestep\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerModel(config)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y, time_features, observed_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                past_values=x.unsqueeze(-1),\n",
    "                past_time_features=time_features.unsqueeze(-1),  # Add time features\n",
    "                past_observed_mask=observed_mask.unsqueeze(-1)   # Add observed mask\n",
    "            ).last_hidden_state.squeeze(-1)\n",
    "            \n",
    "            loss = loss_fn(outputs[:, -prediction_length:], y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, epochs=10)\n",
    "\n",
    "# Inference (Testing)\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for x, y, time_features, observed_mask in test_loader:\n",
    "            outputs = model(\n",
    "                past_values=x.unsqueeze(-1),\n",
    "                past_time_features=time_features.unsqueeze(-1),\n",
    "                past_observed_mask=observed_mask.unsqueeze(-1)\n",
    "            ).last_hidden_state.squeeze(-1)\n",
    "            \n",
    "            predictions.append(outputs[:, -prediction_length:].numpy())\n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model, test_loader)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
