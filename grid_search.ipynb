{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Notebook designed to train models and record results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "  0%|          | 0/188 [00:43<?, ?it/s]\n",
      "  0%|          | 0/144 [00:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 168\u001b[0m\n\u001b[0;32m    146\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.json\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint/\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m15\u001b[39m]\n\u001b[0;32m    163\u001b[0m     }\n\u001b[0;32m    166\u001b[0m     grid_search(param_grid\u001b[38;5;241m=\u001b[39mparam_grid)\n\u001b[1;32m--> 168\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 166\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[0;32m    146\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.json\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint/\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m15\u001b[39m]\n\u001b[0;32m    163\u001b[0m     }\n\u001b[1;32m--> 166\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(param_grid)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     83\u001b[0m     start_epoch \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 84\u001b[0m     train_loss, train_accuracy, train_f1, train_conf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     end_train \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     86\u001b[0m     val_loss, val_accuracy, val_f1, val_conf_matrix \u001b[38;5;241m=\u001b[39m mh\u001b[38;5;241m.\u001b[39mvalidate(model\u001b[38;5;241m=\u001b[39mmodel, val_loader\u001b[38;5;241m=\u001b[39mvalid_loader, optimizer\u001b[38;5;241m=\u001b[39moptimizer, device\u001b[38;5;241m=\u001b[39mdevice, criterion\u001b[38;5;241m=\u001b[39mcriterion)\n",
      "File \u001b[1;32mc:\\Users\\eoinp\\Desktop\\University\\Honours\\implementation\\honours-fin-sentiment\\model_helper.py:125\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, device, criterion)\u001b[0m\n\u001b[0;32m    122\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ts_data, text_data, attention_mask, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m     ts_data \u001b[38;5;241m=\u001b[39m \u001b[43mts_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m    126\u001b[0m     text_data \u001b[38;5;241m=\u001b[39m text_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    127\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import model_helper as mh\n",
    "import data_helper_v2 as dh\n",
    "import torch\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name: str, model, lr):\n",
    "    if optimizer_name.lower() == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_name.lower() == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "    return optimizer\n",
    "\n",
    "def get_criterion(criterion_name:str):#TODO\n",
    "    return torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean'), -1\n",
    "\n",
    "def grid_search(param_grid: dict):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    permutations = list(itertools.product(*param_grid.values()))\n",
    "    combinations = [dict(zip(param_grid.keys(), perm)) for perm in permutations]\n",
    "\n",
    "    best_params = None\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for params in tqdm(combinations, leave=True, position=1):\n",
    "        #====================================================\n",
    "        #general params\n",
    "        #====================================================\n",
    "        out_file                    = params[\"out_file\"]\n",
    "        checkpoint_dir              = params[\"checkpoint_dir\"]\n",
    "        #====================================================\n",
    "        #model params\n",
    "        #====================================================\n",
    "        ts_encoder                  = params[\"ts_encoder\"]\n",
    "        text_encoder                = params[\"text_encoder\"]\n",
    "        projection_dim              = params[\"projection_dim\"]\n",
    "        ts_window                   = params[\"ts_window\"]\n",
    "        \n",
    "        #====================================================\n",
    "        #dataset params\n",
    "        #====================================================\n",
    "        text_window                 = params[\"text_window\"]\n",
    "        data_source                 = params[\"data_source\"]\n",
    "        days_away                   = params[\"days_away\"]\n",
    "        batch_size                  = params[\"batch_size\"]\n",
    "        num_workers                 = params[\"num_workers\"]\n",
    "\n",
    "        #====================================================\n",
    "        #training params\n",
    "        #====================================================\n",
    "        learning_rate               = params[\"learning_rate\"]\n",
    "        optimizer_name              = params[\"optimizer\"]\n",
    "        criterion_name              = params[\"criterion\"]\n",
    "        random_state                = params[\"random_state\"]\n",
    "        num_epochs                  = params[\"num_epochs\"]\n",
    "\n",
    "        \n",
    "        model = mh.get_model(ts_encoder_config=ts_encoder, text_encoder_config=text_encoder, projection_dim=projection_dim, ts_window=ts_window)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer                   = get_optimizer(optimizer_name=optimizer_name, model=model, lr=learning_rate)\n",
    "        criterion, negative_label   = get_criterion(criterion_name=criterion_name)\n",
    "\n",
    "        train_loader, valid_loader, test_loader = dh.get_data(data_source=data_source, model=model, text_window=text_window, ts_window=ts_window, days_away=days_away, negative_label=negative_label, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "        test_loss, test_accuracy, test_f1, test_conf_matrix = None, None, None, None\n",
    "        for epoch in range(num_epochs):\n",
    "            start_epoch = datetime.datetime.now()\n",
    "            train_loss, train_accuracy, train_f1, train_conf_matrix = mh.train(model=model, train_loader=train_loader, optimizer=optimizer, device=device, criterion=criterion)\n",
    "            end_train = datetime.datetime.now()\n",
    "            val_loss, val_accuracy, val_f1, val_conf_matrix = mh.validate(model=model, val_loader=valid_loader, optimizer=optimizer, device=device, criterion=criterion)\n",
    "            end_validate = datetime.datetime.now()\n",
    "            end_epoch = datetime.datetime.now()\n",
    "            \n",
    "            if epoch == num_epochs:\n",
    "                test_loss, test_accuracy, test_f1, test_conf_matrix = mh.validate(model=model, val_loader=test_loader, optimizer=optimizer, device=device, criterion=criterion)      \n",
    "                \n",
    "\n",
    "            data = {\n",
    "                \"end_time\": end_epoch.isoformat(),\n",
    "                \"search_index\": i,\n",
    "                \"epoch\": epoch,\n",
    "                \"model_params\": params,\n",
    "                \"train_metrics\": {\n",
    "                    \"loss\": train_loss,\n",
    "                    \"accuracy\": train_accuracy,\n",
    "                    \"f1\": train_f1,\n",
    "                    \"conf_matrix\": train_conf_matrix\n",
    "                },\n",
    "                \"val_metrics\": {\n",
    "                    \"loss\": val_loss,\n",
    "                    \"accuracy\": val_accuracy,\n",
    "                    \"f1\": val_f1,\n",
    "                    \"conf_matrix\": val_conf_matrix\n",
    "                },\n",
    "                \"test_metrics\": {\n",
    "                    \"loss\": test_loss,\n",
    "                    \"accuracy\": test_accuracy,\n",
    "                    \"f1\": test_f1,\n",
    "                    \"conf_matrix\": test_conf_matrix\n",
    "                },\n",
    "                \"timing\": {\n",
    "                    \"start_epoch\": start_epoch.isoformat(),\n",
    "                    \"end_train\": end_train.isoformat(),\n",
    "                    \"end_validate\": end_validate.isoformat(),\n",
    "                    \"end_epoch_and_test\": end_epoch.isoformat(),\n",
    "                    \"train_time\": (end_train - start_epoch).total_seconds(),\n",
    "                    \"validate_time\": (end_validate - end_train).total_seconds(),\n",
    "                    \"test_time\": (end_epoch - end_validate).total_seconds(),\n",
    "                    \"epoch_time\": (end_epoch - start_epoch).total_seconds()\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write to JSON file\n",
    "            with open(out_file, 'a') as file:\n",
    "                json.dump(data, file)\n",
    "                file.write('\\n')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                print(json.dumps(data, indent=4))\n",
    "                best_val_loss = val_loss\n",
    "                best_params = params\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_search_{i}_time_{end_epoch}_epoch_{epoch}_model_params_{params}.pth\")\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def run():\n",
    "    param_grid = param_grid = {\n",
    "        \"out_file\": ['output.json'],\n",
    "        \"checkpoint_dir\": [\"checkpoint/\"],\n",
    "        \"ts_encoder\": [{\"name\": 'TSTransformerBaseEncoder'}],\n",
    "        \"text_encoder\": [{\"name\": 'bert-base-uncased', \"auto-pre-trained\": True}],\n",
    "        \"projection_dim\": [400, 500, 600, 700],\n",
    "        \"ts_window\": [5],\n",
    "        \"text_window\": [1],\n",
    "        \"data_source\": ['stock_emotions'],\n",
    "        \"days_away\": [31, 60],\n",
    "        \"batch_size\": [16],\n",
    "        \"num_workers\": [6],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.0001, 0.00001],\n",
    "        \"optimizer\": ['adam'],\n",
    "        \"criterion\": ['CosineEmbeddingLoss'],\n",
    "        \"random_state\": [42, 43, 44],\n",
    "        \"num_epochs\": [15]\n",
    "    }\n",
    "\n",
    "\n",
    "    grid_search(param_grid=param_grid)\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
