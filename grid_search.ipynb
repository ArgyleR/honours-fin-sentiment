{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Notebook designed to train models and record results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [00:00<00:00, 38551.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import model_helper as mh\n",
    "import data_helper_v2 as dh\n",
    "import torch\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name: str, model, lr):\n",
    "    if optimizer_name.lower() == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_name.lower() == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "    return optimizer\n",
    "\n",
    "def get_criterion(criterion_name:str):#TODO\n",
    "    return torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean'), -1\n",
    "\n",
    "def grid_search(param_grid: dict):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    permutations = list(itertools.product(*param_grid.values()))\n",
    "    combinations = [dict(zip(param_grid.keys(), perm)) for perm in permutations]\n",
    "\n",
    "    best_params = None\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for params in tqdm(combinations, leave=True, position=1):\n",
    "        #====================================================\n",
    "        #general params\n",
    "        #====================================================\n",
    "        out_file                    = params[\"out_file\"]\n",
    "        checkpoint_dir              = params[\"checkpoint_dir\"]\n",
    "        #====================================================\n",
    "        #model params\n",
    "        #====================================================\n",
    "        ts_encoder                  = params[\"ts_encoder\"]\n",
    "        text_encoder                = params[\"text_encoder\"]\n",
    "        projection_dim              = params[\"projection_dim\"]\n",
    "        ts_window                   = params[\"ts_window\"]\n",
    "        \n",
    "        #====================================================\n",
    "        #dataset params\n",
    "        #====================================================\n",
    "        text_window                 = params[\"text_window\"]\n",
    "        data_source                 = params[\"data_source\"]\n",
    "        days_away                   = params[\"days_away\"]\n",
    "        batch_size                  = params[\"batch_size\"]\n",
    "        num_workers                 = params[\"num_workers\"]\n",
    "\n",
    "        #====================================================\n",
    "        #training params\n",
    "        #====================================================\n",
    "        learning_rate               = params[\"learning_rate\"]\n",
    "        optimizer_name              = params[\"optimizer\"]\n",
    "        criterion_name              = params[\"criterion\"]\n",
    "        random_state                = params[\"random_state\"]\n",
    "        num_epochs                  = params[\"num_epochs\"]\n",
    "\n",
    "        \n",
    "        model = mh.get_model(ts_encoder_config=ts_encoder, text_encoder_config=text_encoder, projection_dim=projection_dim, ts_window=ts_window)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer                   = get_optimizer(optimizer_name=optimizer_name, model=model, lr=learning_rate)\n",
    "        criterion, negative_label   = get_criterion(criterion_name=criterion_name)\n",
    "\n",
    "        train_loader, valid_loader, test_loader = dh.get_data(data_source=data_source, model=model, text_window=text_window, ts_window=ts_window, days_away=days_away, negative_label=negative_label, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "        test_loss, test_accuracy, test_f1, test_conf_matrix = None, None, None, None\n",
    "        for epoch in range(num_epochs):\n",
    "            start_epoch = datetime.datetime.now()\n",
    "            train_loss, train_accuracy, train_f1, train_conf_matrix = mh.train(model=model, train_loader=train_loader, optimizer=optimizer, device=device, criterion=criterion)\n",
    "            end_train = datetime.datetime.now()\n",
    "            val_loss, val_accuracy, val_f1, val_conf_matrix = mh.validate(model=model, val_loader=valid_loader, optimizer=optimizer, device=device, criterion=criterion)\n",
    "            end_validate = datetime.datetime.now()\n",
    "            end_epoch = datetime.datetime.now()\n",
    "            \n",
    "            if epoch == num_epochs:\n",
    "                test_loss, test_accuracy, test_f1, test_conf_matrix = mh.validate(model=model, val_loader=test_loader, optimizer=optimizer, device=device, criterion=criterion)      \n",
    "                \n",
    "\n",
    "            data = {\n",
    "                \"end_time\": end_epoch.isoformat(),\n",
    "                \"search_index\": i,\n",
    "                \"epoch\": epoch,\n",
    "                \"model_params\": params,\n",
    "                \"train_metrics\": {\n",
    "                    \"loss\": train_loss,\n",
    "                    \"accuracy\": train_accuracy,\n",
    "                    \"f1\": train_f1,\n",
    "                    \"conf_matrix\": train_conf_matrix\n",
    "                },\n",
    "                \"val_metrics\": {\n",
    "                    \"loss\": val_loss,\n",
    "                    \"accuracy\": val_accuracy,\n",
    "                    \"f1\": val_f1,\n",
    "                    \"conf_matrix\": val_conf_matrix\n",
    "                },\n",
    "                \"test_metrics\": {\n",
    "                    \"loss\": test_loss,\n",
    "                    \"accuracy\": test_accuracy,\n",
    "                    \"f1\": test_f1,\n",
    "                    \"conf_matrix\": test_conf_matrix\n",
    "                },\n",
    "                \"timing\": {\n",
    "                    \"start_epoch\": start_epoch.isoformat(),\n",
    "                    \"end_train\": end_train.isoformat(),\n",
    "                    \"end_validate\": end_validate.isoformat(),\n",
    "                    \"end_epoch_and_test\": end_epoch.isoformat(),\n",
    "                    \"train_time\": (end_train - start_epoch).total_seconds(),\n",
    "                    \"validate_time\": (end_validate - end_train).total_seconds(),\n",
    "                    \"test_time\": (end_epoch - end_validate).total_seconds(),\n",
    "                    \"epoch_time\": (end_epoch - start_epoch).total_seconds()\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write to JSON file\n",
    "            with open(out_file, 'a') as file:\n",
    "                json.dump(data, file)\n",
    "                file.write('\\n')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                print(json.dumps(data, indent=4))\n",
    "                best_val_loss = val_loss\n",
    "                best_params = params\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_search_{i}_time_{end_epoch}_epoch_{epoch}_model_params_{params}.pth\")\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def run():\n",
    "    param_grid = param_grid = {\n",
    "        \"out_file\": ['output.json'],\n",
    "        \"checkpoint_dir\": [\"checkpoint/\"],\n",
    "        \"ts_encoder\": [{\"name\": 'LSTM'}],\n",
    "        \"text_encoder\": [{\"name\": 'bert-base-uncased', \"auto-pre-trained\": True}],\n",
    "        \"projection_dim\": [64, 32, 16],\n",
    "        \"ts_window\": [5, 7, 10],\n",
    "        \"text_window\": [1, 5],\n",
    "        \"data_source\": ['synthetic'],\n",
    "        \"days_away\": [31, 60],\n",
    "        \"batch_size\": [16],\n",
    "        \"num_workers\": [6],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.0001, 0.00001, 0.000001],\n",
    "        \"optimizer\": ['adam'],\n",
    "        \"criterion\": ['CosineEmbeddingLoss'],\n",
    "        \"random_state\": [42, 43, 44],\n",
    "        \"num_epochs\": [15]\n",
    "    }\n",
    "\n",
    "\n",
    "    grid_search(param_grid=param_grid)\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: -0.7297037292405271\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    # Compute the dot product between the two vectors\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    # Compute the L2 norms (magnitudes) of each vector\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity\n",
    "\n",
    "# Example vectors\n",
    "vec1 = np.array([-4, -5, 0])\n",
    "vec2 = np.array([4, 5, 6])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
