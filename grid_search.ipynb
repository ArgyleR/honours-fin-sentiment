{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Notebook designed to train models and record results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eoinp\\anaconda3\\envs\\deepl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import model_helper as mh\n",
    "import data_helper_v3 as dh3\n",
    "import torch\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed, device):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device != 'cpu':        \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name: str, model, lr):\n",
    "    if optimizer_name.lower() == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_name.lower() == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "    return optimizer\n",
    "\n",
    "def get_criterion(criterion_name:str):#TODO\n",
    "    return torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean'), -1\n",
    "\n",
    "def get_data_base(search_index, epochs, dataset_params, model_params, df_len, pair_count):\n",
    "    #doc is shorted/longer?\n",
    "    data = {\n",
    "        \"end_time\": None,\n",
    "        \"search_index\": search_index,\n",
    "        \"epochs\": epochs,\n",
    "        \"dataset_params\": dataset_params,\n",
    "        \"model_params\": model_params,\n",
    "        \"df_len\": df_len,\n",
    "        \"pair_count\": pair_count,\n",
    "        \"train_metrics\": {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"f1\": [],\n",
    "            \"conf_matrix\": []\n",
    "        },\n",
    "        \"val_metrics\": {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"f1\": [],\n",
    "            \"conf_matrix\": []\n",
    "        },\n",
    "        \"test_metrics\": {\n",
    "            \"loss\": None,\n",
    "            \"accuracy\": None,\n",
    "            \"f1\": None,\n",
    "            \"conf_matrix\": None\n",
    "        },\n",
    "        \"timing\": {\n",
    "            \"start_loop\": None,\n",
    "            \"end_loop\": None,\n",
    "            \"end_test\": None,\n",
    "            \"loop_time\": None,\n",
    "            \"test_time\": None\n",
    "        }\n",
    "    }\n",
    "    data\n",
    "\n",
    "def update_data_train_metrics(data, train_loss, train_acc, train_f1, train_conf_matrix,\n",
    "                                    val_loss, val_acc, val_f1, val_conf_matrix,\n",
    "                                    test_loss, test_acc, test_f1, test_conf_matrix):\n",
    "    data[\"train_metrics\"][\"loss\"] = data[\"train_metrics\"][\"loss\"].append(train_loss)\n",
    "    data[\"train_metrics\"][\"accuracy\"] = data[\"train_metrics\"][\"accuracy\"].append(train_acc)\n",
    "    data[\"train_metrics\"][\"f1\"] = data[\"train_metrics\"][\"f1\"].append(train_f1)\n",
    "    data[\"train_metrics\"][\"conf_matrix\"] = data[\"train_metrics\"][\"conf_matrix\"].append(train_conf_matrix)\n",
    "    data[\"val_metrics\"][\"loss\"] = data[\"val_metrics\"][\"loss\"].append(val_loss)\n",
    "    data[\"val_metrics\"][\"accuracy\"] = data[\"val_metrics\"][\"accuracy\"].append(val_acc)\n",
    "    data[\"val_metrics\"][\"f1\"] = data[\"val_metrics\"][\"f1\"].append(val_f1)\n",
    "    data[\"val_metrics\"][\"conf_matrix\"] = data[\"val_metrics\"][\"conf_matrix\"].append(val_conf_matrix)\n",
    "    data[\"test_metrics\"][\"loss\"] = test_loss\n",
    "    data[\"test_metrics\"][\"accuracy\"] = test_acc\n",
    "    data[\"test_metrics\"][\"f1\"] = test_f1\n",
    "    data[\"test_metrics\"][\"conf_matrix\"] = test_conf_matrix\n",
    "\n",
    "    return data\n",
    "\n",
    "def update_data_timing(data, start_loop, end_loop, end_test, loop_time, test_time):\n",
    "    data['timing']['start_loop'] = start_loop\n",
    "    data['timing']['end_loop'] = end_loop\n",
    "    data['timing']['end_test'] = end_test\n",
    "    data['timing']['loop_time'] = loop_time\n",
    "    data['timing']['test_time'] = test_time\n",
    "\n",
    "    return data\n",
    "            \n",
    "def grid_search(model_param_grid: dict, dataset_param_grid: dict, out_file: str, checkpoint_dir: str, df=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #Datasets take a while to load, load each variation once and perform all model experiments after that\n",
    "    dataset_permutations = list(itertools.product(*dataset_param_grid.values()))\n",
    "    dataset_combinations = [dict(zip(dataset_param_grid.keys(), perm)) for perm in dataset_permutations]\n",
    "\n",
    "    best_dataset_params = None\n",
    "    best_model_params = None\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    i = 1000\n",
    "    for dataset_params in dataset_combinations:        \n",
    "        #====================================================\n",
    "        #dataset params\n",
    "        #====================================================\n",
    "        ts_window                   = dataset_params['ts_window']\n",
    "        ts_overlap                  = dataset_params['ts_overlap']\n",
    "        text_window                 = dataset_params[\"text_window\"]\n",
    "        text_selection_method       = dataset_params['text_selection_method']\n",
    "        data_source                 = dataset_params[\"data_source\"]\n",
    "        negatives_creation          = dataset_params['negatives_creation']\n",
    "        random_state                = dataset_params[\"random_state\"] #effects everything but first call is the ds creation\n",
    "\n",
    "        set_seed(random_state, device=device)\n",
    "\n",
    "        if df is None:\n",
    "            #get the df before anything else: \n",
    "            df = dh3.get_data(model=None, \n",
    "                data_source=data_source, \n",
    "                ts_window=ts_window, \n",
    "                ts_mode=ts_overlap, \n",
    "                text_window=text_window, \n",
    "                text_selection_method=text_selection_method, \n",
    "                negatives_creation=negatives_creation, \n",
    "                batch_size=None, \n",
    "                num_workers=None, \n",
    "                loaders=False)\n",
    "        \n",
    "        df_len = len(df)\n",
    "        \n",
    "        pair_count = df['label'].value_counts()\n",
    "        \n",
    "        model_permutations = list(itertools.product(*model_param_grid.values()))\n",
    "        model_combinations = [dict(zip(model_param_grid.keys(), perm)) for perm in model_permutations]\n",
    "\n",
    "        for model_params in model_combinations:\n",
    "            #====================================================\n",
    "            #model params\n",
    "            #====================================================\n",
    "            ts_encoder                  = model_params[\"ts_encoder\"]\n",
    "            ts_encoder['ts_window']     = ts_window\n",
    "            ts_encoder['context_length']= 2\n",
    "            ts_encoder['prediction_length']=0\n",
    "            ts_encoder['lags_sequence'] = [1, 2, 3]\n",
    "            ts_encoder['num_features']  = 3\n",
    "\n",
    "            text_encoder                = model_params[\"text_encoder\"]\n",
    "            text_encoder_pretrained     = model_params['text_encoder_pretrained']\n",
    "            text_aggregation_method     = model_params['text_aggregation_method']\n",
    "            projection_dim              = model_params[\"projection_dim\"]\n",
    "            ts_window                   = ts_window\n",
    "            batch_size                  = model_params[\"batch_size\"]\n",
    "            num_workers                 = model_params[\"num_workers\"]\n",
    "            \n",
    "            #====================================================\n",
    "            #training params\n",
    "            #====================================================\n",
    "            learning_rate               = model_params[\"learning_rate\"]\n",
    "            optimizer_name              = model_params[\"optimizer\"]\n",
    "            criterion_name              = model_params[\"criterion\"]\n",
    "            num_epochs                  = model_params[\"num_epochs\"]\n",
    "            \n",
    "            model = mh.get_model(ts_encoder_config=ts_encoder, text_encoder_config=text_encoder, projection_dim=projection_dim, ts_window=ts_window, text_aggregation=text_aggregation_method)\n",
    "            model.to(device)\n",
    "\n",
    "            optimizer                   = get_optimizer(optimizer_name=optimizer_name, model=model, lr=learning_rate)\n",
    "            criterion, negative_label   = get_criterion(criterion_name=criterion_name)\n",
    "\n",
    "            df = dh3.correct_negative_labels(df, negative_label=negative_label)\n",
    "            \n",
    "            train_loader, valid_loader, test_loader = dh3.get_data_loaders(df=df, model=model, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "            data = get_data_base(search_index=i, epochs=num_epochs, dataset_params=dataset_params, model_params=model_params, df_len=df_len, pair_count=pair_count)\n",
    "\n",
    "            test_loss, test_accuracy, test_f1, test_conf_matrix = None, None, None, None\n",
    "            start_loop = datetime.datetime.now()\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss, train_accuracy, train_f1, train_conf_matrix = mh.train(model=model, train_loader=train_loader, optimizer=optimizer, device=device, criterion=criterion)\n",
    "                val_loss, val_accuracy, val_f1, val_conf_matrix = mh.validate(model=model, val_loader=valid_loader, optimizer=optimizer, device=device, criterion=criterion)\n",
    "            \n",
    "                data = update_data_train_metrics(data, train_loss, train_accuracy, train_f1, train_conf_matrix,\n",
    "                                    val_loss, val_accuracy, val_f1, val_conf_matrix,\n",
    "                                    test_loss, test_accuracy, test_f1, test_conf_matrix)\n",
    "            \n",
    "            end_loop = datetime.datetime.now()\n",
    "            \n",
    "            test_loss, test_accuracy, test_f1, test_conf_matrix = mh.validate(model=model, val_loader=test_loader, optimizer=optimizer, device=device, criterion=criterion)      \n",
    "            end_test = datetime.datetime.now()\n",
    "            loop_time = (start_loop - end_loop).total_seconds()\n",
    "            test_time= (end_loop - end_test).total_seconds()\n",
    "            data = update_data_timing(data, start_loop, end_loop, end_test, loop_time, test_time)\n",
    "            \n",
    "            # Write to JSON file\n",
    "            with open(out_file, 'a') as file:\n",
    "                json.dump(data, file)\n",
    "                file.write('\\n')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                print(json.dumps(data, indent=4))\n",
    "                best_val_loss = val_loss\n",
    "                best_dataset_params = dataset_params\n",
    "                best_model_params = model_params\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_search_id_{i}.pth\")\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "\n",
    "            i += 1\n",
    "\n",
    "    print(f\"Best Model Params: \\n{best_model_params}\")\n",
    "    print(f\"Best Dataset Params: \\n{best_dataset_params}\")\n",
    "\n",
    "def run(df=None):\n",
    "    #IDEAL PARAM GRID:\n",
    "    model_param_grid = {\n",
    "            \"out_file\": ['output.json'],                                                                                    #GRIDSEARCH     #DONE\n",
    "            \"checkpoint_dir\": [\"checkpoint/\"],                                                                              #GRIDSEARCH     #DONE\n",
    "            \"ts_encoder\": [{\"name\": 'TimeSeriesTransformerModel'}, {\"name\": 'AutoFormer'}, {\"name\": \"InformerModel\"}],        #MODELhelper\n",
    "            \"text_encoder\": [{\"name\": 'bert-base-uncased'}, {\"name\": 'bert-base'}],                                         #MODELhelper\n",
    "            \"text_encoder_pretrained\": [True, False],                                                                       #MODELhelper\n",
    "            \"text_aggregation_method\": [\"mean\", \"max\", \"string_append\"],                                                    #MODELhelper\n",
    "            \"projection_dim\": [400, 500, 600, 700],                                                                         #MODELhelper\n",
    "            \"learning_rate\": [0.00001, 0.0001],                                                                             #GRIDSEARCH     #DONE\n",
    "            \"optimizer\": ['adam'],                                                                                          #GRIDSEARCH     #DONE\n",
    "            \"criterion\": ['CosineEmbeddingLoss'],                                                                           #GRIDSEARCH     #DONEISH                                                   \n",
    "            \"num_epochs\": [10],                                                                                             #GRIDSEARCH     #DONE\n",
    "            \"batch_size\": [16],                                                                                             #DATAhelper     #DONE\n",
    "            \"num_workers\": [6],  \n",
    "        }\n",
    "\n",
    "    dataset_param_grid = {\n",
    "        \"text_selection_method\": [\"TFIDF\"],                                                                             #DATAhelper\n",
    "        \"ts_window\": [5],                                                                         #DATAhelper\n",
    "        \"ts_overlap\": ['start', 'middle', 'end'],                                                                    #DATAhelper\n",
    "        \"text_window\": [3],                                                                 #DATAhelper\n",
    "        'text_selection_method': [('TFIDF', 5)],\n",
    "        \"data_source\": [{\n",
    "            \"name\": \"stock_net\",\n",
    "            \"text_path\": \"./data/stocknet-dataset/tweet/organised_tweet.csv\",\n",
    "            \"ts_path\": \"./data/stocknet-dataset/price/raw/\",\n",
    "            \"ts_date_col\": 'Date',\n",
    "            'text_date_col': 'created_at'\n",
    "        }],                                                            #DATAhelper\n",
    "        \"negatives_creation\": [(\"naive\", 31), (\"naive\", 60), (\"diff_distribution\", )],                          #DATAhelper\n",
    "        \"random_state\": [42, 43, 44],\n",
    "    }\n",
    "\n",
    "    grid_search(model_param_grid=model_param_grid, dataset_param_grid=dataset_param_grid, out_file='output_temp.json', checkpoint_dir='checkpoint_temp/', df=df)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping window for ticker D on 2015-08-14: empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    }
   ],
   "source": [
    "import data_helper_v3 as dh3\n",
    "data_config = {\n",
    "    \"name\": \"stock_net\",\n",
    "    \"text_path\": \"./data/stocknet-dataset/tweet/organised_tweet.csv\",\n",
    "    \"ts_path\": \"./data/stocknet-dataset/price/raw/\",\n",
    "    \"ts_date_col\": 'Date',\n",
    "    'text_date_col': 'created_at'\n",
    "}\n",
    "df = dh3.get_data(data_source=data_config, model=None, text_window=3, loaders=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
